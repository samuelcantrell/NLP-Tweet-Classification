{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Tweet Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5a2fi7JePkG",
        "colab_type": "text"
      },
      "source": [
        "## Import Neccessary Packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyilNou3d0Ax",
        "colab_type": "code",
        "outputId": "6301dd34-232e-4a02-85cc-8f2db71fae52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  # Returns split words, while tf vectorizes after splitting.\n",
        "\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xAKlUqfqgn",
        "colab_type": "code",
        "outputId": "8eaf5d73-dcba-49c9-e9f5-e62e50fff2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (46.0.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (2.1.0.dev202003290105)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.21.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.38.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.21.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly) (46.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\n",
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8CjWN0FdTax",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQc0rSfb94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(uncleaned_tweet):\n",
        "    # Order of operations:\n",
        "    # 1. Remove symbols/emojis.\n",
        "    # 2. Tokenize for subsequent processes.\n",
        "    # 3. Remove http links.\n",
        "    # 4. Replace contractions with root words.\n",
        "    # 5. Remove @whatever.\n",
        "    # 6. Separate and Remove '_', '#', '/', ''' in that order.\n",
        "    # 7. Join and return.\n",
        "\n",
        "    # Identify junk to be removed.\n",
        "    symbols = ['*', '+', '...', '?', '`', '``', '[', ']', '(', ')', '-', '~', '|']\n",
        "    \n",
        "    emotes = ['\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ',\n",
        "              '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ', '\\x89ÛÒ', 'Ûªs']    \n",
        "    \n",
        "\n",
        "    def remove_symbols(uncleaned_tweet):\n",
        "        for s in symbols:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(s, ' ')\n",
        "        uncleaned_tweet = uncleaned_tweet.replace('&', ' and ')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_symbols(uncleaned_tweet)\n",
        "    \n",
        "    \n",
        "    def remove_emote(uncleaned_tweet):\n",
        "        for emote in emotes:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(emote, '')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_emote(uncleaned_tweet)\n",
        "    word_tokens = word_tokenize(uncleaned_tweet)\n",
        "\n",
        "\n",
        "    def remove_links(word_tokens):\n",
        "        if 'http' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('http')]\n",
        "        if 'https' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('https')]\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_links(word_tokens)\n",
        "\n",
        "\n",
        "    def delete_contraction(word_tokens):\n",
        "        for i, w in enumerate(word_tokens):\n",
        "            if w == \"'d\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"n't\":\n",
        "                word_tokens[i] = 'not'\n",
        "            elif w == \"'m\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'re\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == 'gon':\n",
        "                word_tokens[i] = 'going to'\n",
        "            elif w == 'na':\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'s\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'ll\":\n",
        "                word_tokens[i] = ''\n",
        "            else:\n",
        "                pass\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = delete_contraction(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_at(word_tokens):  # remove '@' and words followed by @\n",
        "        j = 0\n",
        "        for i in range(len(word_tokens)):\n",
        "            try:\n",
        "                if word_tokens[j] == '@':\n",
        "                    del word_tokens[j + 1]\n",
        "                    del word_tokens[j]\n",
        "                    j += -1\n",
        "                j += 1\n",
        "            except:\n",
        "                break\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_at(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_numbers(word_tokens):\n",
        "      j = 0\n",
        "      for i in range(len(word_tokens)):\n",
        "        try:\n",
        "          if word_tokens[i-j].isnumeric():\n",
        "            del word_tokens[i-j]\n",
        "            j += 1\n",
        "        except:\n",
        "          break\n",
        "      return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_numbers(word_tokens)\n",
        "\n",
        "    cleaned_tweet = ' '.join(word_tokens).replace('_', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('#', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('/', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace(\"'\", ' ')\n",
        "\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n",
        "def clean_tweet_dataframe(df):  # Jaki's, not used but more efficient\n",
        "  data = df.string.values\n",
        "  clean_data = []\n",
        "  ######################## Denoising Texts #########################\n",
        "  letters = {'A', 'B', 'C', 'E', 'F', 'G', 'H', 'M', 'N', 'P', 'S', 'V', 'Y', 'a', 'b', 'v'}\n",
        "  symbols = {'!', '#', '$', '&', \"'\", '(', ')', '*', '+', '-', '_', '.', '/', ':', ';', '=', '?', '`', '|', '~',\n",
        "            '[', ']', '^',\n",
        "            '\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ', '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ',\n",
        "            '\\x89ÛÒ', 'Ûªs', 'Û_'}\n",
        "  start = time.time()\n",
        "  for d in data:\n",
        "      ws = d.split(' ')\n",
        "      for i, w in enumerate(ws):\n",
        "          # process 1: remove https and @\n",
        "          if ('http' in w) or ('@' in w):\n",
        "              ws[i] = ''\n",
        "          # process 2: remove single letters\n",
        "          if w in letters:\n",
        "              ws[i] = ''\n",
        "      new_d = ' '.join(ws)\n",
        "      # process 3: remove symbols and emotes\n",
        "      for s in symbols:\n",
        "          new_d = new_d.replace(s, ' ')\n",
        "      # process 4: replace multiple spaces with one space\n",
        "      new_d = ' '.join(new_d.split())\n",
        "      clean_data.append(new_d.lower())\n",
        "  end = time.time() - start\n",
        "  print(end)\n",
        "  print(len(clean_data))\n",
        "  return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlEDnkdWd1FI",
        "colab_type": "text"
      },
      "source": [
        "## Spell-Checker and Remove_Stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwj599LheNmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spellcheck(input_words):  # Additionally casts to LC and removes excess spaces as a nice side-effect of how it spell-checks.\n",
        "  speller = SpellChecker()\n",
        "  words = speller.split_words(input_words)\n",
        "  [speller.correction(w) for w in words]\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "def remove_stopwords(input_words):\n",
        "  stop_words = set(stopwords.words('english'))  # List of stopwords to remove\n",
        "  word_tokens = word_tokenize(input_words)\n",
        "  words = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "  return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZIoaXKfIwm",
        "colab_type": "text"
      },
      "source": [
        "## Other Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "souan2Eg7QEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dim(input_tweets):\n",
        "  m = 0\n",
        "  for i in range(len(input_tweets)):\n",
        "    word_tokens = word_tokenize(input_tweets[i])\n",
        "    m = max(m, len(word_tokens))\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_usfa7gtekIK",
        "colab_type": "text"
      },
      "source": [
        "## Data Import and Pre-Processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PZNUK32oqSD",
        "colab_type": "code",
        "outputId": "19474a6c-0075-4de2-dffe-0fdbbab868b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Try to import the cleaned sheet, if it doesn't exist, re-clean the raw data\n",
        "\n",
        "try:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  data = pd.read_csv(input_file).to_numpy()\n",
        "  print(\"Cleaned data successfully read from file!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Ugly_words_FULL - Sheet1.csv\"\n",
        "  df = pd.read_csv(input_file)\n",
        "  labels = df.columns.values\n",
        "  X = df.to_numpy()\n",
        "  Y = X[:1500, 0:6].astype(int) # Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News?\n",
        "  X = X[:1500, 6].astype(str)  # Dimensionless to pass strings/elements, expand to do matrix operations\n",
        "  # print(Y.shape)\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = clean_tweet(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = spellcheck(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = remove_stopwords(X[i])\n",
        "\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  X = np.expand_dims(X, axis=1)\n",
        "  data = np.concatenate((Y, X), axis=1)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.columns = labels\n",
        "\n",
        "  output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  df.to_csv(output_file, header=True, index=False)\n",
        "\n",
        "  print(\"Cleaned data successfully written to file!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaned data successfully read from file!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlHf_xz1GB6S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e1f49362-4e01-4701-c13f-9150b15c1a58"
      },
      "source": [
        "# Oversample if desired (probably yes).\n",
        "y_start = 3\n",
        "y_end = 4\n",
        "\n",
        "oversample = True\n",
        "num_copies = 3  # Number of copies to add\n",
        "\n",
        "if oversample:\n",
        "  minority = data[:, :][data[:, y_start] == 1]\n",
        "  print(minority.shape)\n",
        "  \n",
        "  for i in range(num_copies):\n",
        "    np.random.shuffle(minority)\n",
        "    temp = minority\n",
        "    data = np.append(data, temp[:200, :], axis=0)\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(213, 7)\n",
            "(2100, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZt_7s5DRb0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "11bcadbd-133a-4a29-ab32-236a6355b75a"
      },
      "source": [
        "# Check sizes and split into train and test sets.\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News? [0:4]\n",
        "Y_train = data[:1100, y_start:y_end].astype(int).squeeze()\n",
        "X_train = data[:1100, 6].astype(str)\n",
        "Y_valid = data[1100:1600, y_start:y_end].astype(int).squeeze()\n",
        "X_valid = data[1100:1600, 6].astype(str)\n",
        "Y_test = data[1600:2100, y_start:y_end].astype(int).squeeze()\n",
        "X_test = data[1600:2100, 6].astype(str)\n",
        "\n",
        "# Print splits.\n",
        "print(len(X_train[Y_train==1]))\n",
        "print(len(X_valid[Y_valid==1]))\n",
        "print(len(X_test[Y_test==1]))\n",
        "\n",
        "# Tokenize and standardize lengths for training:\n",
        "token_len = find_dim(data[:, 6].astype(str))\n",
        "print(\"Padding Length: \" + str(token_len))\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.fit_on_texts(X_valid)\n",
        "# tokenizer.fit_on_texts(X_test)  # Don't fit over test since it's not part of the model.\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)  # Vectorizes into numbers\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)  # Vectorizes into numbers\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)  # Vectorizes into numbers\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=token_len, padding='pre', truncating='post')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "431\n",
            "200\n",
            "182\n",
            "Padding Length: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8Xp1mIeojP",
        "colab_type": "text"
      },
      "source": [
        "## Tensorization and Embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSk44mu0j5uD",
        "colab_type": "code",
        "outputId": "39e34f7d-7d29-4f97-9c0e-075c8a205c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Tensorization, shuffle and batch so the format is correct for embedding.\n",
        "X_train = tf.constant(X_train, dtype=tf.int64)\n",
        "Y_train = tf.constant(Y_train, dtype=tf.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(100).batch(100)\n",
        "print(train_data)\n",
        "\n",
        "X_valid = tf.constant(X_valid, dtype=tf.int64)\n",
        "Y_valid = tf.constant(Y_valid, dtype=tf.int64)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid)).shuffle(100).batch(100)\n",
        "print(valid_data)\n",
        "\n",
        "X_test = tf.constant(X_test, dtype=tf.int64)\n",
        "Y_test = tf.constant(Y_test, dtype=tf.int64)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(100).batch(100)\n",
        "print(test_data)\n",
        "\n",
        "# print(list(test_data.as_numpy_iterator()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEXAb3fe3qA",
        "colab_type": "text"
      },
      "source": [
        "## Build and Train Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc6Y16rCgXHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "fd66567c-ccda-46f6-b7db-b3317d4939df"
      },
      "source": [
        "# Define model layers with Dense over LSTM for Classification\n",
        "print(len(tokenizer.word_index))\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 1000))\n",
        "# Flatten to 1D to line up with subsequent layers since Embedding can take multiple and Dense cannot\n",
        "model.add(tf.keras.layers.LSTM(100, activation=\"relu\", return_sequences=False, recurrent_dropout=0.1))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4782\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 1000)        4783000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               440400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 5,253,801\n",
            "Trainable params: 5,253,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9nUvMtjijR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "800ec2e0-b70b-406e-b795-b40aab58485f"
      },
      "source": [
        "# Define model parameters after defining layers with compile:\n",
        "tf.keras.backend.clear_session()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Run the fitting and train the model:\n",
        "# batch_size = 100 # Number of observations run per set\n",
        "history = model.fit(train_data, validation_data=valid_data, epochs=100, verbose=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 2s 172ms/step - loss: 0.6795 - accuracy: 0.6055 - val_loss: 0.6519 - val_accuracy: 0.6000\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.5184 - accuracy: 0.6900 - val_loss: 0.3774 - val_accuracy: 0.8600\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.3316 - accuracy: 0.9182 - val_loss: 0.4383 - val_accuracy: 0.8040\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.1546 - accuracy: 0.9773 - val_loss: 0.2776 - val_accuracy: 0.8860\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0499 - accuracy: 0.9864 - val_loss: 0.2931 - val_accuracy: 0.9020\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0307 - accuracy: 0.9900 - val_loss: 0.3014 - val_accuracy: 0.9200\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0232 - accuracy: 0.9891 - val_loss: 0.3244 - val_accuracy: 0.9240\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0206 - accuracy: 0.9909 - val_loss: 0.3679 - val_accuracy: 0.9300\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0167 - accuracy: 0.9909 - val_loss: 0.3877 - val_accuracy: 0.9360\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0175 - accuracy: 0.9927 - val_loss: 0.3938 - val_accuracy: 0.9360\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0165 - accuracy: 0.9936 - val_loss: 0.4223 - val_accuracy: 0.9380\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0156 - accuracy: 0.9927 - val_loss: 0.4475 - val_accuracy: 0.9400\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0153 - accuracy: 0.9936 - val_loss: 0.4525 - val_accuracy: 0.9400\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0154 - accuracy: 0.9936 - val_loss: 0.4611 - val_accuracy: 0.9400\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0149 - accuracy: 0.9936 - val_loss: 0.4779 - val_accuracy: 0.9360\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0148 - accuracy: 0.9936 - val_loss: 0.4782 - val_accuracy: 0.9400\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0152 - accuracy: 0.9936 - val_loss: 0.4897 - val_accuracy: 0.9400\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0148 - accuracy: 0.9936 - val_loss: 0.5183 - val_accuracy: 0.9380\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 0.5355 - val_accuracy: 0.9400\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 0.5442 - val_accuracy: 0.9400\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0144 - accuracy: 0.9936 - val_loss: 0.5925 - val_accuracy: 0.9400\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 0.5977 - val_accuracy: 0.9400\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 0.5986 - val_accuracy: 0.9400\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 0.6194 - val_accuracy: 0.9400\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 0.6132 - val_accuracy: 0.9400\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0144 - accuracy: 0.9936 - val_loss: 0.6290 - val_accuracy: 0.9400\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0144 - accuracy: 0.9936 - val_loss: 0.6643 - val_accuracy: 0.9400\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 0.6582 - val_accuracy: 0.9400\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 0.6624 - val_accuracy: 0.9400\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 0.6840 - val_accuracy: 0.9400\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0144 - accuracy: 0.9936 - val_loss: 0.6933 - val_accuracy: 0.9400\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 0.7107 - val_accuracy: 0.9400\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 0.7094 - val_accuracy: 0.9400\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 0.7533 - val_accuracy: 0.9400\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 0.7745 - val_accuracy: 0.9400\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 0.7881 - val_accuracy: 0.9400\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 0.8048 - val_accuracy: 0.9380\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 0.8455 - val_accuracy: 0.9400\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0144 - accuracy: 0.9936 - val_loss: 0.8664 - val_accuracy: 0.9360\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 0.8706 - val_accuracy: 0.9400\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 0.9068 - val_accuracy: 0.9400\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 0.9310 - val_accuracy: 0.9360\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 0.9467 - val_accuracy: 0.9360\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 0.9772 - val_accuracy: 0.9360\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.0048 - val_accuracy: 0.9360\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.0403 - val_accuracy: 0.9360\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.0625 - val_accuracy: 0.9360\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.0503 - val_accuracy: 0.9380\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.0764 - val_accuracy: 0.9360\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.0945 - val_accuracy: 0.9360\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 1.1060 - val_accuracy: 0.9380\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.1529 - val_accuracy: 0.9360\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.1700 - val_accuracy: 0.9360\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.1371 - val_accuracy: 0.9380\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0139 - accuracy: 0.9936 - val_loss: 1.1881 - val_accuracy: 0.9400\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.2348 - val_accuracy: 0.9400\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0145 - accuracy: 0.9936 - val_loss: 1.2693 - val_accuracy: 0.9380\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.2253 - val_accuracy: 0.9400\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.2759 - val_accuracy: 0.9380\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.3143 - val_accuracy: 0.9360\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.3811 - val_accuracy: 0.9340\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.3785 - val_accuracy: 0.9340\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.4238 - val_accuracy: 0.9360\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.4847 - val_accuracy: 0.9360\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.5691 - val_accuracy: 0.9340\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0139 - accuracy: 0.9936 - val_loss: 1.6207 - val_accuracy: 0.9360\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.7011 - val_accuracy: 0.9380\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.7483 - val_accuracy: 0.9380\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.8057 - val_accuracy: 0.9380\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.7552 - val_accuracy: 0.9400\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 1.7272 - val_accuracy: 0.9360\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.7601 - val_accuracy: 0.9340\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.7917 - val_accuracy: 0.9340\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 1.8323 - val_accuracy: 0.9340\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0138 - accuracy: 0.9936 - val_loss: 1.9123 - val_accuracy: 0.9360\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0139 - accuracy: 0.9936 - val_loss: 2.0350 - val_accuracy: 0.9360\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 2.0182 - val_accuracy: 0.9380\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 1.9737 - val_accuracy: 0.9380\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 2.0400 - val_accuracy: 0.9360\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 2.1189 - val_accuracy: 0.9360\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 2.1327 - val_accuracy: 0.9360\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 2.1495 - val_accuracy: 0.9360\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 2.2678 - val_accuracy: 0.9360\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 2.3263 - val_accuracy: 0.9340\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 2.4644 - val_accuracy: 0.9340\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 2.5978 - val_accuracy: 0.9360\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0139 - accuracy: 0.9936 - val_loss: 2.7182 - val_accuracy: 0.9360\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 2.8272 - val_accuracy: 0.9360\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 2.8808 - val_accuracy: 0.9360\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0139 - accuracy: 0.9936 - val_loss: 2.8692 - val_accuracy: 0.9360\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 3.0416 - val_accuracy: 0.9360\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 3.0509 - val_accuracy: 0.9360\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 3.0967 - val_accuracy: 0.9340\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 3.1605 - val_accuracy: 0.9320\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 3.0824 - val_accuracy: 0.9320\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 3.2611 - val_accuracy: 0.9320\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 3.4717 - val_accuracy: 0.9320\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 3.3713 - val_accuracy: 0.9320\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 3.3454 - val_accuracy: 0.9320\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0140 - accuracy: 0.9936 - val_loss: 3.5507 - val_accuracy: 0.9320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS0CEZ1me5S4",
        "colab_type": "text"
      },
      "source": [
        "## Predict and Metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z95XMfjtiPP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e48b4ca4-6b2c-468e-8715-40260d51641d"
      },
      "source": [
        "results = model.evaluate(test_data, verbose=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 - 0s - loss: 0.9028 - accuracy: 0.9340\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZo1fqLiLj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d0617fc3-1bf2-4fdb-f1f6-3eef75f23a7d"
      },
      "source": [
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.903\n",
            "accuracy: 0.934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSs6J5Aiyf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b3ca036e-10e0-4a12-bbf0-27211dad8f5e"
      },
      "source": [
        "\"\"\"\n",
        "test_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\n",
        "test_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\n",
        "test_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase1)))\n",
        "\n",
        "test_phrase2 = \"urgent news from israel\"\n",
        "test_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\n",
        "test_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase2)))\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\\ntest_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\\ntest_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase1)))\\n\\ntest_phrase2 = \"urgent news from israel\"\\ntest_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\\ntest_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase2)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLpc0QLRxQ4q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "3a09f349-7508-43cd-a51f-7a8a569aaaa6"
      },
      "source": [
        "Y_hat = model.predict_classes(X_test).reshape(len(Y_test), 1)\n",
        "print(len(Y_hat))\n",
        "print(type(Y_hat))\n",
        "print(len(Y_hat[Y_hat==1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-be29c51753dd>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "500\n",
            "<class 'numpy.ndarray'>\n",
            "183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4uHbgPD7eGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "868b330c-ff19-429d-8f8a-23b9efaed90d"
      },
      "source": [
        "# Make Confusion Matrix\n",
        "plt.figure(figsize=(9, 6))\n",
        "y_true = pd.Series(Y_test.numpy().squeeze(), name=\"Actual Label\")\n",
        "y_pred = pd.Series(Y_hat.squeeze(), name=\"Predicted Label\")\n",
        "sns.heatmap(pd.crosstab(y_true, y_pred), annot=True, fmt=\"d\", linewidths=0.25)\n",
        "plt.ylim(len(set(Y_test.numpy())), 0)  # Fix limits, matplotlib bugged (ver. 3.11)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.0, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAF2CAYAAAAcHvCGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAe2ElEQVR4nO3de5QddZXo8e8mD3kGiIGsGKK8AoqM\nAkIEBAZEUBAvoFyUcZRxmAmjqOBwXeA46zIoXnFEGEREgjyCvBUEZADFiAP4AIIEDK8xE0ASAyG8\nElBIOr3vH12RQ0x3n9TJ6XO66vvJqpVzflWnajdrhbN7//avKjITSZJUT2t1OgBJktQ5JgKSJNWY\niYAkSTVmIiBJUo2ZCEiSVGMmApIk1ZiJgCRJXSoi1o6IuyLivoh4ICJOLsa3iIg7I2JORFwZEaOL\n8dcV7+cU+zcf7BomApIkda9XgHdn5tuBHYD3RcSuwNeAMzJza+A54Kji+KOA54rxM4rjBmQiIElS\nl8o+LxZvRxVbAu8GflCMTwcOKV4fXLyn2L9vRMRA1xi5RiNes3LZormdjkHqSqPGbQnAyNETOxyJ\n1J16ls4HGPALcE1YtmhuS7fnHTVuy0FjjIgRwD3A1sDZwP8Az2dmT3HIPGDF/wwmAk8AZGZPRLwA\nvB5Y1N/5uzkRkCSpu/Uub+njETEVmNowNC0zpzUek5nLgR0iYiPgh8CbW7roSkwEJEnqkOJLf9qg\nB/Yd+3xE3ArsBmwUESOLqsBmwPzisPnAJGBeRIwENgSeGei89ghIklRW9ra2DSIiNikqAUTEOsB+\nwEPArcBhxWFHAtcVr68v3lPs/1kO8nRBKwKSJJXVO/iXeYsmANOLPoG1gKsy84aIeBC4IiJOAe4F\nzi+OPx/4XkTMAZ4FPjLYBUwEJEkqKZv4rb618+f9wI6rGJ8LTFnF+MvA/16dazg1IElSjVkRkCSp\nrPZPDbSdiYAkSWW1eWpgKJgISJJUVov3EegGJgKSJJVVgYqAzYKSJNWYFQFJksqyWVCSpPpq930E\nhoKJgCRJZVkRkCSpxipQEbBZUJKkGrMiIElSWd5HQJKkGqvA1ICJgCRJZVWgWdAeAUmSasyKgCRJ\nZTk1IElSjVVgasBEQJKkkjJdNSBJUn1VYGrAZkFJkmrMioAkSWXZIyBJUo1VYGrARECSpLK8xbAk\nSTVWgYqAzYKSJNWYFQFJksqyWVCSpBqrwNSAiYAkSWVVoCJgj4AkSTVmRUCSpLIqUBEwEZAkqSQf\nOiRJUp1ZEZAkqcYqsGrAZkFJkmrMioAkSWU5NSBJUo1VYGrARECSpLKsCEiSVGMVqAjYLChJUo1Z\nEZAkqSynBiRJqjETAUmSasweAUmSNJxZEZAkqSynBiRJqrEKTA2YCEiSVJYVAUmSaqwCFQGbBSVJ\nqjETAUmSyurtbW0bRERMiohbI+LBiHggIo4txv8tIuZHxKxiO7DhM1+IiDkR8UhEvHewazg1IElS\nWe3vEegBjs/M30TEBsA9EXFLse+MzDyt8eCI2A74CPBW4A3ATyNim8xc3t8FrAhIklRWZmvboKfP\nBZn5m+L1EuAhYOIAHzkYuCIzX8nMR4E5wJSBrmEiIElSWW2eGmgUEZsDOwJ3FkOfjoj7I+KCiNi4\nGJsIPNHwsXkMnDiYCEiS1CkRMTUiZjZsU/s5bn3gauC4zFwMnANsBewALAC+UTYGewQkSSqrxR6B\nzJwGTBvomIgYRV8ScGlmXlN87qmG/ecBNxRv5wOTGj6+WTHWLysCkiSVlb2tbYOIiADOBx7KzNMb\nxic0HHYoMLt4fT3wkYh4XURsAUwG7hroGlYEJEkqq/2rBt4FfAz4bUTMKsb+BTgiInYAEngMOBog\nMx+IiKuAB+lbcXDMQCsGwERAkqSulZl3ALGKXTcO8JmvAF9p9homApIkldXEEsBuZyIgSVJZPnRI\nkqQaMxGQJKnGfPqgJEkazqwISJJUUvbaLChJUn3ZIyBJUo1VoEfARECSpLIqMDVgs6AkSTVmRUCS\npLLsEZAkqcZMBCRJqrEKPGvAHgFJkmrMioAAeOWVpRx5zOdZumwZy3uWs98+e/Dpf/gY8/7wJJ8/\n6VSef2Ex2207mVP/7/9h1KhRzJz1W7525rn89/88ytdPPpH999mz0z+CNGTOm/YN3n/ge1j49CJ2\n2HFfAC679By22WYrADbacAzPv7CYnXfZv5NhaihUYGrAioAAGD16FBd881Sumf5tfjD9bH5x5z3c\nN/shzjjnAj724UO46aoLGLPB+lx9w48BmDB+U0754vEcuN8+HY5cGnoXX3wV7z/oo68Z+5uPfpKd\nd9mfnXfZnx/+8Eauvbbfx8WrSnqzta0LtC0RiIg3R8QJEfHNYjshIt7SruupNRHBuuuuA0BPTw89\nPT1EBHfecx/779332/7BB76Hn932KwAmThjPtltvwVoRHYtZ6pTb77iTZ597vt/9hx32Aa648roh\njEgdk72tbV2gLYlARJwAXAEEcFexBXB5RJzYjmuqdcuXL+dDRx7DXgcdwW677MikiRPYYP31GDly\nBADjNxnHwqef6XCUUnfbc4938tTCp5kz59FOh6KhUIGKQLt6BI4C3pqZyxoHI+J04AHg1FV9KCKm\nAlMBzj33XD7xwfe0KTytyogRI7h6+tksXvIix37hyzz6+BOdDkkadj784UO40mqAhpF2JQK9wBuA\nx1can1DsW6XMnAZMW/F22aK57YlOAxqzwfpM2eltzJr9MEtefImenuWMHDmCp55exKabvL7T4Uld\na8SIERx6yAFM2fWAToeiIZI2C/brOGBGRNwUEdOK7WZgBnBsm66pFjz73PMsXvIiAC+/8gq/uvte\nttx8ElN2ehs/+fntAFx340959567dTJMqau9Z989eeSROcyfv6DToWioODWwapl5c0RsA0wBJhbD\n84G7M3N5O66p1jz9zHN88ZTTWN7bS/Ym7333nuz9rney1eZv5PMnncpZ0y7mLdtsxQcP6lsO9duH\nHuG4L3yZxUte5Oe/uJOzv3sJ1116bod/CmloXPK9s/nrvXZj3LixPDZ3Jid/6TQuvOgKDj/8YJsE\n66ZLGv5aEdm9d0VyakDqx6hxWwIwcvTEQY6U6qln6Xzoa1Jvq5dO+duWvkTX+9dLOr70yhsKSZJU\nVpeU91thIiBJUlkVaBY0EZAkqSwrApIk1VgFmgV91oAkSTVmRUCSpLKcGpAkqb6qcGdBEwFJksqy\nIiBJUo1VIBGwWVCSpBqzIiBJUlkVWD5oIiBJUlkVmBowEZAkqaSsQCJgj4AkSTVmRUCSpLIqUBEw\nEZAkqSxvKCRJUo1ZEZAkqcYqkAjYLChJUo1ZEZAkqaTM4V8RMBGQJKmsCkwNmAhIklSWiYAkSfXl\nnQUlSdKwZkVAkqSyKlARMBGQJKms4X9jQacGJEkqK3uzpW0wETEpIm6NiAcj4oGIOLYYHxsRt0TE\n74q/Ny7GIyK+GRFzIuL+iNhpsGuYCEiS1L16gOMzcztgV+CYiNgOOBGYkZmTgRnFe4ADgMnFNhU4\nZ7ALmAhIklRWb7a2DSIzF2Tmb4rXS4CHgInAwcD04rDpwCHF64OBi7PPr4GNImLCQNcwEZAkqaze\n1raImBoRMxu2qf1dKiI2B3YE7gTGZ+aCYteTwPji9UTgiYaPzSvG+mWzoCRJJbV6H4HMnAZMG+y4\niFgfuBo4LjMXR0TjOTIiSgdiIiBJUllDsGogIkbRlwRcmpnXFMNPRcSEzFxQlP4XFuPzgUkNH9+s\nGOuXUwOSJHWp6PvV/3zgocw8vWHX9cCRxesjgesaxj9erB7YFXihYQphlawISJJU0hDcYvhdwMeA\n30bErGLsX4BTgasi4ijgceDwYt+NwIHAHOCPwCcGu4CJgCRJZbV5aiAz7wCin937ruL4BI5ZnWuY\nCEiSVFJW4M6CJgKSJJVVgUTAZkFJkmrMioAkSSU5NSBJUp2ZCEiSVF9VqAjYIyBJUo1ZEZAkqaQq\nVARMBCRJKslEQJKkOsv+bvo3fJgISJJUUhUqAjYLSpJUY1YEJEkqKXsrPDUQEWcB/T5fMTM/25aI\nJEkaJqowNTBQRWDmkEUhSdIwlFVuFszM6Y3vI2LdzPxj+0OSJGl4qEJFYNBmwYjYLSIeBB4u3r89\nIr7d9sgkSVLbNbNq4D+A9wLPAGTmfcBe7QxKkqThIHujpa0bNLVqIDOfiHhNwMvbE44kScNH9ttS\nP3w0kwg8ERG7AxkRo4BjgYfaG5YkSd2vW36rb0UzUwP/BBwDTAT+AOxQvJckScPcoBWBzFwEfHQI\nYpEkaVipRUUgIraMiB9FxNMRsTAirouILYciOEmSullma1s3aGZq4DLgKmAC8Abg+8Dl7QxKkqTh\noAqrBppJBNbNzO9lZk+xXQKs3e7AJEnqdpnR0tYNBnrWwNji5U0RcSJwBX3PHvgwcOMQxCZJktps\noGbBe+j74l+RshzdsC+BL7QrKEmShoMq3GJ4oGcNbDGUgUiSNNz0dkl5vxVN3VkwIrYHtqOhNyAz\nL25XUJIkDQfdMs/fikETgYg4CdibvkTgRuAA4A7ARECSVGvd0vnfimZWDRwG7As8mZmfAN4ObNjW\nqCRJ0pBoZmrgT5nZGxE9ETEGWAhManNckiR1vW65KVArmkkEZkbERsB59K0keBH4VVujkiRpGKjC\n1EAzzxr4VPHyOxFxMzAGWNTWqCRJGgZqs2pghcx8DCAifg+8sR0BSZKkobNaiUCD4Z8CSZLUolos\nH+xHBdojJElqTaWbBSPiLFb9hR/ARm2LSJKkYaLqPQIzS+6TJKkWKj01kJnThzIQSZI09Mr2CEiS\nVHuV7hHoBqPGbdnpEKSu1rN0fqdDkGqt6j0CHTdy9MROhyB1pRUJwOwtD+pwJFJ32n7uDUNynUr3\nCAywagCAzPxsWyKSJGmYqHpFwJUBkiRVnKsGJEkqqQK9goP3CETEJsAJwHbA2ivGM/PdbYxLkqSu\nV4WpgbWaOOZS4CFgC+Bk4DHg7jbGJEnSsJAZLW3doJlE4PWZeT6wLDP/KzP/HrAaIEnSEIiICyJi\nYUTMbhj7t4iYHxGziu3Ahn1fiIg5EfFIRLx3sPM3s3xwWfH3goh4P/AHYOzq/iCSJFVN79Bc5iLg\nW8DFK42fkZmnNQ5ExHbAR4C3Am8AfhoR22Tm8v5O3kwicEpEbAgcD5wFjAE+13T4kiRVVNL+8n5m\n3hYRmzd5+MHAFZn5CvBoRMwBpgC/6u8DgyYCmbnirgwvAPs0GYgkSZXX2+KygYiYCkxtGJqWmdOa\n/PinI+Lj9C33Pz4znwMmAr9uOGZeMdavZlYNXMgqVkgUvQKSJNVWb4sVgeJLv9kv/kbnAF+m7/v5\ny8A3gFLfy81MDTTep3Ft4FD6+gQkSVIHZOZTK15HxHm8+l09H5jUcOhmxVi/mpkauLrxfURcDtzR\nbLCSJFXVUPQIrEpETMjMBcXbQ4EVKwquBy6LiNPpaxacDNw10LnKPHRoMrBpic9JklQpQ7FqoPgF\nfG9gXETMA04C9o6IHeibGngMOBogMx+IiKuAB4Ee4JiBVgxAcz0CS3htj8CT9N1pUJKkWhuiVQNH\nrGL4/AGO/wrwlWbP38zUwAbNnkySJA0vg95ZMCJmNDMmSVLd9La4dYN+KwIRsTawLn1zEhvDn+sf\nYxhkTaIkSXXQLV/mrRhoauBo4Dj6ug7v4dVEYDF9tzqUJKnWOrVqYE3qNxHIzDOBMyPiM5l51hDG\nJEnSsNA7/POApp4+2BsRG614ExEbR8Sn2hiTJEkaIs0kAv+Ymc+veFPcy/gf2xeSJEnDQy/R0tYN\nmrmh0IiIiMxMgIgYAYxub1iSJHW/Fp851BWaSQRuBq6MiHOL90cXY5Ik1VrVVw2scAJ9j0j8ZPH+\nFuC8tkUkSdIw0RvdUd5vxaA9ApnZm5nfyczDMvMw+u5f7CoCSZIqoKmHDkXEjsARwOHAo8A17QxK\nkqThoNI9AhGxDX1f/kcAi4ArgcjMfYYoNkmSulrVewQeBm4HDsrMOQAR8bkhiUqSpGGg6jcU+iCw\nALg1Is6LiH2hSxY9SpKkNaLfRCAzr83MjwBvBm6l77kDm0bEORGx/1AFKElSt6rCDYWaWTXwUmZe\nlpkfADYD7qVvSaEkSbWWLW7doKlVAysUtxeeVmySJNVaFXoEVisRkCRJr6rCqoFmHjokSZIqyoqA\nJEkldcs8fytMBCRJKskeAUmSaqwKPQImApIklVSFRMBmQUmSasyKgCRJJaU9ApIk1VcVpgZMBCRJ\nKqkKiYA9ApIk1ZgVAUmSSvKGQpIk1Zg3FJIkqcaq0CNgIiBJUklVSARsFpQkqcasCEiSVJLNgpIk\n1ZjNgpIk1VgVegRMBCRJKqkKUwM2C0qSVGNWBCRJKqm3AjUBEwFJkkqyR0CSpBob/vUAewQkSao1\nKwKSJJXk1IAkSTXmDYUkSaoxVw1IklRjwz8NsFlQkqRaMxGQJKmk3ha3ZkTEBRGxMCJmN4yNjYhb\nIuJ3xd8bF+MREd+MiDkRcX9E7DTY+U0EJEkqqZdsaWvSRcD7Vho7EZiRmZOBGcV7gAOAycU2FThn\nsJObCEiSVFK2uDV1jczbgGdXGj4YmF68ng4c0jB+cfb5NbBRREwY6Pw2C0qSVFIH7yMwPjMXFK+f\nBMYXrycCTzQcN68YW0A/rAhIktQhETE1ImY2bFNX9xyZuToFhr9gRUCSpJJavY9AZk4DppX46FMR\nMSEzFxSl/4XF+HxgUsNxmxVj/bIiIElSSUPRI9CP64Eji9dHAtc1jH+8WD2wK/BCwxTCKlkRkCSp\npKHoEYiIy4G9gXERMQ84CTgVuCoijgIeBw4vDr8ROBCYA/wR+MRg5zcRkCSpi2XmEf3s2ncVxyZw\nzOqc30RAkqSSsgI3GTYRkCSpJB9DLElSjfn0QUmSamz4pwEuH5QkqdZMBPQXzpv2Df4w7z5m3Tvj\nNePHfOoTzP7tf3HfrJ9x6le/2KHopM6Y+LVjefNdl7D1TWe/Znzsxw9i8i3nsPXNZzP+hFdXar3u\nzZuz5Q9OY+ubz2brm75FjB411CFrCAzRQ4fayqkB/YWLL76Kb3/7Qi688Mw/j+3917vzvz7wXnZ6\nx34sXbqUTTZ5fQcjlIbecz/4Kc9cfAObnfbPfx5bb9e/Ysx+uzLn/Z8hl/Yw4vUb9u0YsRaTTj+e\nef98Oi8//CgjNtqA7FneocjVTlVoFrQioL9w+x138uxzz79m7OijP86/f/1sli5dCsDTTz/TidCk\njvnj3Q+w/Pklrxkb+9EDefo73yeX9gCw/JkXAFh/z514+eHHePnhR/vGn18CvVX4ytDKssU/3WDI\nE4GIGPQuR+o+kydvyR57TOGXd/yIn/30B+z8jrd3OiSp40ZvMZH1dnkrW17zDba4/Kus87bJALxu\nizdAJm+66Etsdf1/MG7qhzocqdqlt8WtG3SiInByfzsan8I0bVqZZzCoXUaOHMHGG2/E7nt8gBNO\nPIXLL/tOp0OSOi5GjGDEhhsw94PH8+RXL2TSWSf07RgxgnV33o55nzuNuYefwJj9d2O93U2e1Z3a\n0iMQEff3t4tXn5n8F1Z6ClN+6tP95gwaYvPnLeDaa28C4O6Zs+jt7WXcuLEsWvRshyOTOmfZk4tY\n/ONfAvCn+/8bepMRY8fQ8+QzvHTXAyx/bjEAS34+k3XeuhUv/fK+ToarNuiW8n4r2lURGA98HPjA\nKjYnl4eh667/MXvvvTvQN00wevRokwDV3uJbfs16u74NgNFbvIEYNZLlzy5myW33sPa2byLWfh2M\nWIv13rk9L8/5fYejVTtUYWqgXasGbgDWz8xZK++IiJ+36ZpaQy753tn89V67MW7cWB6bO5OTv3Qa\nF150Bd897xvMuncGS5cu4++POq7TYUpDarMzP8967/wrRm48hm1/cRELz7yU579/CxO/dixb33Q2\nuWwZ8z5/BgC9i19i0fnXstW1p0P2VQRevHVmZ38AtUVvDv+KQGT3/hA5cvTETscgdaWepfMBmL3l\nQR2OROpO28+9Afqmo9vqY2/6YEtfot97/Jq2xzgY7yMgSVJJXfur9GowEZAkqaRuuTtgK0wEJEkq\nqQqrBkwEJEkqqVs6/1vhLYYlSaoxKwKSJJVkj4AkSTVmj4AkSTVWhR4BEwFJkkrq4pvyNc1mQUmS\nasyKgCRJJdksKElSjdkjIElSjVVh1YA9ApIk1ZgVAUmSSrJHQJKkGqvC8kETAUmSSrJZUJKkGrNZ\nUJIkDWtWBCRJKslmQUmSasxmQUmSaqwKFQF7BCRJqjErApIklVSFVQMmApIkldRrj4AkSfU1/NMA\nEwFJkkqzWVCSJA1rVgQkSSqpChUBEwFJkkryhkKSJNWYFQFJkmqsCvcRsFlQkqQasyIgSVJJ9ghI\nklRjQ9EjEBGPAUuA5UBPZu4cEWOBK4HNgceAwzPzuTLnd2pAkqSSMrOlbTXsk5k7ZObOxfsTgRmZ\nORmYUbwvxURAkqTh52BgevF6OnBI2ROZCEiSVFIv2dLWpAR+EhH3RMTUYmx8Zi4oXj8JjC/7M9gj\nIElSSa0uHyy+2Kc2DE3LzGkrHbZHZs6PiE2BWyLi4dfEkJkRUToQEwFJkkpq9THExZf+yl/8Kx8z\nv/h7YUT8EJgCPBUREzJzQURMABaWjcGpAUmSSsoW/wwmItaLiA1WvAb2B2YD1wNHFocdCVxX9mew\nIiBJUvcaD/wwIqDvO/uyzLw5Iu4GroqIo4DHgcPLXsBEQJKkklqdGhhMZs4F3r6K8WeAfdfENUwE\nJEkqqQrPGjARkCSppHZXBIaCiYAkSSVVoSLgqgFJkmrMioAkSSU5NSBJUo1VYWrARECSpJIyezsd\nQsvsEZAkqcasCEiSVNJqPEGwa5kISJJUUtosKElSfVkRkCSpxqpQEbBZUJKkGrMiIElSSd5QSJKk\nGvOGQpIk1VgVegRMBCRJKqkKqwZsFpQkqcasCEiSVJJTA5Ik1ZirBiRJqrEqVATsEZAkqcasCEiS\nVFIVVg2YCEiSVFIVpgZMBCRJKslmQUmSaqwKtxi2WVCSpBqzIiBJUklODUiSVGM2C0qSVGNV6BEw\nEZAkqaQqVARsFpQkqcasCEiSVFIVKgLRxT9E1wYmSRoWot0XGDl6YkvfVT1L57c9xsF0cyKgLhMR\nUzNzWqfjkLqV/0Y0HNkjoNUxtdMBSF3OfyMadkwEJEmqMRMBSZJqzERAq8O5T2lg/hvRsGOzoCRJ\nNWZFQJKkGjMR0KAi4n0R8UhEzImIEzsdj9RNIuKCiFgYEbM7HYtUhomABhQRI4CzgQOA7YAjImK7\nzkYldZWLgPd1OgipLBMBDWYKMCcz52bmUuAK4OAOxyR1jcy8DXi203FIZZkIaDATgSca3s8rxiRJ\nFWAiIElSjZkIaDDzgUkN7zcrxiRJFWAioMHcDUyOiC0iYjTwEeD6DsckSVpDTAQ0oMzsAT4N/Bh4\nCLgqMx/obFRS94iIy4FfAdtGxLyIOKrTMUmrwzsLSpJUY1YEJEmqMRMBSZJqzERAkqQaMxGQJKnG\nTAQkSaoxEwGpCRGxPCJmRcTsiPh+RKzbwrkuiojDitffHeghThGxd0TsXuIaj0XEuGbH+znH30XE\nt9bEdSV1LxMBqTl/yswdMnN7YCnwT407I2JkmZNm5j9k5oMDHLI3sNqJgCQ1y0RAWn23A1sXv63f\nHhHXAw9GxIiI+HpE3B0R90fE0QDR51sR8UhE/BTYdMWJIuLnEbFz8fp9EfGbiLgvImZExOb0JRyf\nK6oRe0bEJhFxdXGNuyPiXcVnXx8RP4mIByLiu0A0+8NExJSI+FVE3BsRv4yIbRt2Typi/F1EnNTw\nmb+NiLuKuM4tHlctaRgq9VuMVFfFb/4HADcXQzsB22fmoxExFXghM3eJiNcBv4iInwA7AtsC2wHj\ngQeBC1Y67ybAecBexbnGZuazEfEd4MXMPK047jLgjMy8IyLeSN8dH98CnATckZlfioj3A6tzd7uH\ngT0zsyci3gP8P+BDxb4pwPbAH4G7I+I/gZeADwPvysxlEfFt4KPAxatxTUldwkRAas46ETGreH07\ncD59Jfu7MvPRYnx/4G0r5v+BDYHJwF7A5Zm5HPhDRPxsFeffFbhtxbkys7/n278H2C7iz7/wj4mI\n9YtrfLD47H9GxHOr8bNtCEyPiMlAAqMa9t2Smc8ARMQ1wB5AD/AO+hIDgHWAhatxPUldxERAas6f\nMnOHxoHiS/ClxiHgM5n545WOO3ANxrEWsGtmvryKWMr6MnBrZh5aTEf8vGHfyvcgT/p+zumZ+YVW\nLiqpO9gjIK05PwY+GRGjACJim4hYD7gN+HDRQzAB2GcVn/01sFdEbFF8dmwxvgTYoOG4nwCfWfEm\nIlYkJ7cBf1OMHQBsvBpxb8irj5b+u5X27RcRYyNiHeAQ4BfADOCwiNh0RawR8abVuJ6kLmIiIK05\n36Vv/v83ETEbOJe+qtsPgd8V+y6m70l1r5GZTwNTgWsi4j7gymLXj4BDVzQLAp8Fdi6aER/k1dUL\nJ9OXSDxA3xTB7weI8/7iKXnzIuJ04N+Br0bEvfxllfAu4GrgfuDqzJxZrHL4V+AnEXE/cAswocn/\nRpK6jE8flCSpxqwISJJUYyYCkiTVmImAJEk1ZiIgSVKNmQhIklRjJgKSJNWYiYAkSTVmIiBJUo39\nfzOrzAtHjwH6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ8ktCDu6tWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = np.expand_dims(X, axis=1)\n",
        "output = np.concatenate((Y_hat, Y_test.numpy().reshape(-1, 1), data[1600:2100, 6:7].reshape(-1, 1)), axis=1)\n",
        "df = pd.DataFrame(output)\n",
        "\n",
        "output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/NLP_Tweet_Classification_Results.csv\"\n",
        "df.to_csv(output_file, header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLNQXNktgyf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}