{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Tweet Classification_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5a2fi7JePkG",
        "colab_type": "text"
      },
      "source": [
        "## Import Neccessary Packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyilNou3d0Ax",
        "colab_type": "code",
        "outputId": "08218efa-ec9c-4238-fad7-a2806d7153d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  # Returns split words, while tf vectorizes after splitting.\n",
        "\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/d1/ec4e830e9f9c1fd788e1459dd09279fdf807bc7a475579fd7192450b879c/pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xAKlUqfqgn",
        "colab_type": "code",
        "outputId": "f197e32c-ee70-4cad-9107-9840e3683df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (46.0.0)\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/91/fdacaa63403345dc4732b197dcb29bb86a374c60de6686bb0b2f74de8868/tfds_nightly-2.1.0.dev202003290105-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.38.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.21.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.21.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly) (46.0.0)\n",
            "Installing collected packages: tfds-nightly\n",
            "Successfully installed tfds-nightly-2.1.0.dev202003290105\n",
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8CjWN0FdTax",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQc0rSfb94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(uncleaned_tweet):\n",
        "    # Order of operations:\n",
        "    # 1. Remove symbols/emojis.\n",
        "    # 2. Tokenize for subsequent processes.\n",
        "    # 3. Remove http links.\n",
        "    # 4. Replace contractions with root words.\n",
        "    # 5. Remove @whatever.\n",
        "    # 6. Separate and Remove '_', '#', '/', ''' in that order.\n",
        "    # 7. Join and return.\n",
        "\n",
        "    # Identify junk to be removed.\n",
        "    symbols = ['*', '+', '...', '?', '`', '``', '[', ']', '(', ')', '-', '~', '|']\n",
        "    \n",
        "    emotes = ['\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ',\n",
        "              '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ', '\\x89ÛÒ', 'Ûªs']    \n",
        "    \n",
        "\n",
        "    def remove_symbols(uncleaned_tweet):\n",
        "        for s in symbols:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(s, ' ')\n",
        "        uncleaned_tweet = uncleaned_tweet.replace('&', ' and ')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_symbols(uncleaned_tweet)\n",
        "    \n",
        "    \n",
        "    def remove_emote(uncleaned_tweet):\n",
        "        for emote in emotes:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(emote, '')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_emote(uncleaned_tweet)\n",
        "    word_tokens = word_tokenize(uncleaned_tweet)\n",
        "\n",
        "\n",
        "    def remove_links(word_tokens):\n",
        "        if 'http' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('http')]\n",
        "        if 'https' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('https')]\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_links(word_tokens)\n",
        "\n",
        "\n",
        "    def delete_contraction(word_tokens):\n",
        "        for i, w in enumerate(word_tokens):\n",
        "            if w == \"'d\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"n't\":\n",
        "                word_tokens[i] = 'not'\n",
        "            elif w == \"'m\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'re\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == 'gon':\n",
        "                word_tokens[i] = 'going to'\n",
        "            elif w == 'na':\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'s\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'ll\":\n",
        "                word_tokens[i] = ''\n",
        "            else:\n",
        "                pass\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = delete_contraction(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_at(word_tokens):  # remove '@' and words followed by @\n",
        "        j = 0\n",
        "        for i in range(len(word_tokens)):\n",
        "            try:\n",
        "                if word_tokens[j] == '@':\n",
        "                    del word_tokens[j + 1]\n",
        "                    del word_tokens[j]\n",
        "                    j += -1\n",
        "                j += 1\n",
        "            except:\n",
        "                break\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_at(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_numbers(word_tokens):\n",
        "      j = 0\n",
        "      for i in range(len(word_tokens)):\n",
        "        try:\n",
        "          if word_tokens[i-j].isnumeric():\n",
        "            del word_tokens[i-j]\n",
        "            j += 1\n",
        "        except:\n",
        "          break\n",
        "      return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_numbers(word_tokens)\n",
        "\n",
        "    cleaned_tweet = ' '.join(word_tokens).replace('_', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('#', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('/', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace(\"'\", ' ')\n",
        "\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n",
        "def clean_tweet_dataframe(df):  # Jaki's, not used but more efficient\n",
        "  data = df.string.values\n",
        "  clean_data = []\n",
        "  ######################## Denoising Texts #########################\n",
        "  letters = {'A', 'B', 'C', 'E', 'F', 'G', 'H', 'M', 'N', 'P', 'S', 'V', 'Y', 'a', 'b', 'v'}\n",
        "  symbols = {'!', '#', '$', '&', \"'\", '(', ')', '*', '+', '-', '_', '.', '/', ':', ';', '=', '?', '`', '|', '~',\n",
        "            '[', ']', '^',\n",
        "            '\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ', '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ',\n",
        "            '\\x89ÛÒ', 'Ûªs', 'Û_'}\n",
        "  start = time.time()\n",
        "  for d in data:\n",
        "      ws = d.split(' ')\n",
        "      for i, w in enumerate(ws):\n",
        "          # process 1: remove https and @\n",
        "          if ('http' in w) or ('@' in w):\n",
        "              ws[i] = ''\n",
        "          # process 2: remove single letters\n",
        "          if w in letters:\n",
        "              ws[i] = ''\n",
        "      new_d = ' '.join(ws)\n",
        "      # process 3: remove symbols and emotes\n",
        "      for s in symbols:\n",
        "          new_d = new_d.replace(s, ' ')\n",
        "      # process 4: replace multiple spaces with one space\n",
        "      new_d = ' '.join(new_d.split())\n",
        "      clean_data.append(new_d.lower())\n",
        "  end = time.time() - start\n",
        "  print(end)\n",
        "  print(len(clean_data))\n",
        "  return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlEDnkdWd1FI",
        "colab_type": "text"
      },
      "source": [
        "## Spell-Checker and Remove_Stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwj599LheNmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spellcheck(input_words):  # Additionally casts to LC and removes excess spaces as a nice side-effect of how it spell-checks.\n",
        "  speller = SpellChecker()\n",
        "  words = speller.split_words(input_words)\n",
        "  [speller.correction(w) for w in words]\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "def remove_stopwords(input_words):\n",
        "  stop_words = set(stopwords.words('english'))  # List of stopwords to remove\n",
        "  word_tokens = word_tokenize(input_words)\n",
        "  words = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "  return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZIoaXKfIwm",
        "colab_type": "text"
      },
      "source": [
        "## Other Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "souan2Eg7QEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dim(input_tweets):\n",
        "  m = 0\n",
        "  for i in range(len(input_tweets)):\n",
        "    word_tokens = word_tokenize(input_tweets[i])\n",
        "    m = max(m, len(word_tokens))\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_usfa7gtekIK",
        "colab_type": "text"
      },
      "source": [
        "## Data Import and Pre-Processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PZNUK32oqSD",
        "colab_type": "code",
        "outputId": "bcce21c0-8248-46e5-8ff8-95902837952e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Try to import the cleaned sheet, if it doesn't exist, re-clean the raw data\n",
        "\n",
        "try:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  data = pd.read_csv(input_file).to_numpy()\n",
        "  print(\"Cleaned data successfully read from file!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Ugly_words_FULL - Sheet1.csv\"\n",
        "  df = pd.read_csv(input_file)\n",
        "  labels = df.columns.values\n",
        "  X = df.to_numpy()\n",
        "  Y = X[:1500, 0:6].astype(int) # Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News?\n",
        "  X = X[:1500, 6].astype(str)  # Dimensionless to pass strings/elements, expand to do matrix operations\n",
        "  # print(Y.shape)\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = clean_tweet(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = spellcheck(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = remove_stopwords(X[i])\n",
        "\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  X = np.expand_dims(X, axis=1)\n",
        "  data = np.concatenate((Y, X), axis=1)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.columns = labels\n",
        "\n",
        "  output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  df.to_csv(output_file, header=True, index=False)\n",
        "\n",
        "  print(\"Cleaned data successfully written to file!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaned data successfully read from file!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlHf_xz1GB6S",
        "colab_type": "code",
        "outputId": "93187c8c-e299-4bcc-de96-bf777bb36a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Oversample if desired (probably yes).\n",
        "y_start = 4\n",
        "y_end = 5\n",
        "\n",
        "oversample = True\n",
        "num_copies = 3  # Number of copies to add\n",
        "\n",
        "if oversample:\n",
        "  minority = data[:, :][data[:, y_start] == 1]\n",
        "  print(minority.shape)\n",
        "  \n",
        "  for i in range(num_copies):\n",
        "    np.random.shuffle(minority)\n",
        "    temp = minority\n",
        "    data = np.append(data, temp[:200, :], axis=0)\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(220, 7)\n",
            "(2100, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZt_7s5DRb0T",
        "colab_type": "code",
        "outputId": "d0dbf79a-5b19-4b33-fae3-48c6cf97e0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Check sizes and split into train and test sets.\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News? [0:4]\n",
        "Y_train = data[:1100, y_start:y_end].astype(int).squeeze()\n",
        "X_train = data[:1100, 6].astype(str)\n",
        "Y_valid = data[1100:1600, y_start:y_end].astype(int).squeeze()\n",
        "X_valid = data[1100:1600, 6].astype(str)\n",
        "Y_test = data[1600:2100, y_start:y_end].astype(int).squeeze()\n",
        "X_test = data[1600:2100, 6].astype(str)\n",
        "\n",
        "# Print splits.\n",
        "print(len(X_train[Y_train==1]))\n",
        "print(len(X_valid[Y_valid==1]))\n",
        "print(len(X_test[Y_test==1]))\n",
        "\n",
        "# Tokenize and standardize lengths for training:\n",
        "token_len = find_dim(data[:, 6].astype(str))\n",
        "print(\"Padding Length: \" + str(token_len))\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.fit_on_texts(X_valid)\n",
        "# tokenizer.fit_on_texts(X_test)  # Don't fit over test since it's not part of the model.\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)  # Vectorizes into numbers\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)  # Vectorizes into numbers\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)  # Vectorizes into numbers\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=token_len, padding='pre', truncating='post')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "436\n",
            "196\n",
            "188\n",
            "Padding Length: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8Xp1mIeojP",
        "colab_type": "text"
      },
      "source": [
        "## Tensorization and Embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSk44mu0j5uD",
        "colab_type": "code",
        "outputId": "108d45b0-4aa6-444a-dffb-afd2cffd8666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Tensorization, shuffle and batch so the format is correct for embedding.\n",
        "X_train = tf.constant(X_train, dtype=tf.int64)\n",
        "Y_train = tf.constant(Y_train, dtype=tf.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(100).batch(100)\n",
        "print(train_data)\n",
        "\n",
        "X_valid = tf.constant(X_valid, dtype=tf.int64)\n",
        "Y_valid = tf.constant(Y_valid, dtype=tf.int64)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid)).shuffle(100).batch(100)\n",
        "print(valid_data)\n",
        "\n",
        "X_test = tf.constant(X_test, dtype=tf.int64)\n",
        "Y_test = tf.constant(Y_test, dtype=tf.int64)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(100).batch(100)\n",
        "print(test_data)\n",
        "\n",
        "# print(list(test_data.as_numpy_iterator()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEXAb3fe3qA",
        "colab_type": "text"
      },
      "source": [
        "## Build and Train Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc6Y16rCgXHz",
        "colab_type": "code",
        "outputId": "18c5c86a-ffad-484a-9806-a4bcd09122df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Define model layers with Dense over LSTM for Classification\n",
        "print(len(tokenizer.word_index))\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 1000))\n",
        "# Flatten to 1D to line up with subsequent layers since Embedding can take multiple and Dense cannot\n",
        "model.add(tf.keras.layers.LSTM(100, activation=\"relu\", return_sequences=False, recurrent_dropout=0.1))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4893\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 1000)        4894000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               440400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 5,364,801\n",
            "Trainable params: 5,364,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9nUvMtjijR",
        "colab_type": "code",
        "outputId": "73f9a4f2-e026-4dc3-875b-7ffd033b330a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define model parameters after defining layers with compile:\n",
        "tf.keras.backend.clear_session()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Run the fitting and train the model:\n",
        "# batch_size = 100 # Number of observations run per set\n",
        "history = model.fit(train_data, validation_data=valid_data, epochs=100, verbose=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 0.6813 - accuracy: 0.6027 - val_loss: 0.6426 - val_accuracy: 0.6080\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.5866 - accuracy: 0.6064 - val_loss: 0.4931 - val_accuracy: 0.7040\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.2239 - accuracy: 0.9400 - val_loss: 0.2307 - val_accuracy: 0.9300\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0328 - accuracy: 0.9936 - val_loss: 0.3887 - val_accuracy: 0.8960\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.1798 - accuracy: 0.9964 - val_loss: 0.5703 - val_accuracy: 0.8040\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0369 - accuracy: 0.9909 - val_loss: 0.5118 - val_accuracy: 0.7820\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0065 - accuracy: 0.9991 - val_loss: 0.3047 - val_accuracy: 0.8660\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.2739 - val_accuracy: 0.8880\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.2767 - val_accuracy: 0.8900\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.2893 - val_accuracy: 0.8920\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0020 - accuracy: 0.9982 - val_loss: 0.3042 - val_accuracy: 0.8880\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0018 - accuracy: 0.9982 - val_loss: 0.3186 - val_accuracy: 0.8860\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0017 - accuracy: 0.9982 - val_loss: 0.3311 - val_accuracy: 0.8880\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0016 - accuracy: 0.9982 - val_loss: 0.3407 - val_accuracy: 0.8880\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.3475 - val_accuracy: 0.8880\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.3540 - val_accuracy: 0.8880\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0016 - accuracy: 0.9982 - val_loss: 0.3592 - val_accuracy: 0.8860\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.3652 - val_accuracy: 0.8880\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0017 - accuracy: 0.9982 - val_loss: 0.3665 - val_accuracy: 0.8880\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.3732 - val_accuracy: 0.8880\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0018 - accuracy: 0.9982 - val_loss: 0.3727 - val_accuracy: 0.8880\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.3787 - val_accuracy: 0.8900\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.3813 - val_accuracy: 0.8920\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0016 - accuracy: 0.9982 - val_loss: 0.3828 - val_accuracy: 0.8900\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.3867 - val_accuracy: 0.8920\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.3868 - val_accuracy: 0.8920\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0016 - accuracy: 0.9982 - val_loss: 0.3864 - val_accuracy: 0.8900\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.3887 - val_accuracy: 0.8920\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.3902 - val_accuracy: 0.8900\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.3938 - val_accuracy: 0.8920\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.3975 - val_accuracy: 0.8920\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.3987 - val_accuracy: 0.8920\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.3989 - val_accuracy: 0.8900\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4021 - val_accuracy: 0.8920\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4041 - val_accuracy: 0.8920\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.4026 - val_accuracy: 0.8900\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.4054 - val_accuracy: 0.8920\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4067 - val_accuracy: 0.8920\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.4088 - val_accuracy: 0.8920\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4076 - val_accuracy: 0.8920\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.4055 - val_accuracy: 0.8900\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4076 - val_accuracy: 0.8920\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4108 - val_accuracy: 0.8920\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.4117 - val_accuracy: 0.8920\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.4114 - val_accuracy: 0.8920\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.4127 - val_accuracy: 0.8920\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4139 - val_accuracy: 0.8920\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4145 - val_accuracy: 0.8920\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4167 - val_accuracy: 0.8920\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4194 - val_accuracy: 0.8920\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4174 - val_accuracy: 0.8900\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4200 - val_accuracy: 0.8900\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4213 - val_accuracy: 0.8900\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4234 - val_accuracy: 0.8920\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4246 - val_accuracy: 0.8920\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4268 - val_accuracy: 0.8920\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.4254 - val_accuracy: 0.8900\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4281 - val_accuracy: 0.8920\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4296 - val_accuracy: 0.8920\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4265 - val_accuracy: 0.8920\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4280 - val_accuracy: 0.8920\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.4268 - val_accuracy: 0.8940\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.4292 - val_accuracy: 0.8920\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4321 - val_accuracy: 0.8920\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4345 - val_accuracy: 0.8920\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.4352 - val_accuracy: 0.8940\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4373 - val_accuracy: 0.8920\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4390 - val_accuracy: 0.8940\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4410 - val_accuracy: 0.8920\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4425 - val_accuracy: 0.8920\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4327 - val_accuracy: 0.8960\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4304 - val_accuracy: 0.8980\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4299 - val_accuracy: 0.8960\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4308 - val_accuracy: 0.8960\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4339 - val_accuracy: 0.8940\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.4349 - val_accuracy: 0.8960\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4333 - val_accuracy: 0.8960\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4367 - val_accuracy: 0.8960\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4379 - val_accuracy: 0.8960\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4374 - val_accuracy: 0.8960\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.4378 - val_accuracy: 0.8980\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4376 - val_accuracy: 0.8980\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4375 - val_accuracy: 0.8980\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4391 - val_accuracy: 0.8980\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4403 - val_accuracy: 0.8980\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4401 - val_accuracy: 0.8980\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4404 - val_accuracy: 0.8960\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4447 - val_accuracy: 0.8980\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4459 - val_accuracy: 0.8980\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4469 - val_accuracy: 0.8980\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0015 - accuracy: 0.9982 - val_loss: 0.4428 - val_accuracy: 0.8960\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4462 - val_accuracy: 0.8960\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4521 - val_accuracy: 0.8960\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4525 - val_accuracy: 0.8980\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4508 - val_accuracy: 0.8980\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4518 - val_accuracy: 0.8960\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0014 - accuracy: 0.9982 - val_loss: 0.4503 - val_accuracy: 0.8960\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4549 - val_accuracy: 0.8980\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4564 - val_accuracy: 0.8980\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0013 - accuracy: 0.9982 - val_loss: 0.4575 - val_accuracy: 0.8980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS0CEZ1me5S4",
        "colab_type": "text"
      },
      "source": [
        "## Predict and Metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z95XMfjtiPP0",
        "colab_type": "code",
        "outputId": "e29184c4-8537-40c5-b9af-bf358abd1e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model.evaluate(test_data, verbose=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 - 0s - loss: 0.2635 - accuracy: 0.9520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZo1fqLiLj-",
        "colab_type": "code",
        "outputId": "f7b45870-965f-4cda-fd04-3096732cb5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.263\n",
            "accuracy: 0.952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSs6J5Aiyf3",
        "colab_type": "code",
        "outputId": "6bc2d97a-a376-4505-ae57-9c7fa80cad2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "test_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\n",
        "test_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\n",
        "test_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase1)))\n",
        "\n",
        "test_phrase2 = \"urgent news from israel\"\n",
        "test_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\n",
        "test_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase2)))\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\\ntest_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\\ntest_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase1)))\\n\\ntest_phrase2 = \"urgent news from israel\"\\ntest_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\\ntest_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase2)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLpc0QLRxQ4q",
        "colab_type": "code",
        "outputId": "bde10891-5206-4cae-b45d-99528f98d42d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "Y_hat = model.predict_classes(X_test).reshape(len(Y_test), 1)\n",
        "print(len(Y_hat))\n",
        "print(type(Y_hat))\n",
        "print(len(Y_hat[Y_hat==1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-be29c51753dd>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "500\n",
            "<class 'numpy.ndarray'>\n",
            "198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4uHbgPD7eGD",
        "colab_type": "code",
        "outputId": "57bd77ff-9378-49f9-e9b8-d8b2f6e300f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Make Confusion Matrix\n",
        "plt.figure(figsize=(9, 6))\n",
        "y_true = pd.Series(Y_test.numpy().squeeze(), name=\"Actual Label\")\n",
        "y_pred = pd.Series(Y_hat.squeeze(), name=\"Predicted Label\")\n",
        "sns.heatmap(pd.crosstab(y_true, y_pred), annot=True, fmt=\"d\", linewidths=0.25)\n",
        "plt.ylim(len(set(Y_test.numpy())), 0)  # Fix limits, matplotlib bugged (ver. 3.11)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.0, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAFzCAYAAABM02E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ8ktCDu6tWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = np.expand_dims(X, axis=1)\n",
        "output = np.concatenate((Y_hat, Y_test.numpy().reshape(-1, 1), data[1600:2100, 6:7].reshape(-1, 1)), axis=1)\n",
        "df = pd.DataFrame(output)\n",
        "\n",
        "output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/NLP_Tweet_Classification_Results.csv\"\n",
        "df.to_csv(output_file, header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLNQXNktgyf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}