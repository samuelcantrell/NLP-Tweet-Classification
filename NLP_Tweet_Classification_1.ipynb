{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Tweet Classification_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5a2fi7JePkG",
        "colab_type": "text"
      },
      "source": [
        "## Import Neccessary Packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyilNou3d0Ax",
        "colab_type": "code",
        "outputId": "a51644f5-b0fb-4c56-c6cb-ac7b03791075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  # Returns split words, while tf vectorizes after splitting.\n",
        "\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xAKlUqfqgn",
        "colab_type": "code",
        "outputId": "8adc0523-af7c-4936-dccc-ae090b0af0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (46.0.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (2.1.0.dev202003290105)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.21.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.38.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly) (46.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8CjWN0FdTax",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQc0rSfb94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(uncleaned_tweet):\n",
        "    # Order of operations:\n",
        "    # 1. Remove symbols/emojis.\n",
        "    # 2. Tokenize for subsequent processes.\n",
        "    # 3. Remove http links.\n",
        "    # 4. Replace contractions with root words.\n",
        "    # 5. Remove @whatever.\n",
        "    # 6. Separate and Remove '_', '#', '/', ''' in that order.\n",
        "    # 7. Join and return.\n",
        "\n",
        "    # Identify junk to be removed.\n",
        "    symbols = ['*', '+', '...', '?', '`', '``', '[', ']', '(', ')', '-', '~', '|']\n",
        "    \n",
        "    emotes = ['\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ',\n",
        "              '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ', '\\x89ÛÒ', 'Ûªs']    \n",
        "    \n",
        "\n",
        "    def remove_symbols(uncleaned_tweet):\n",
        "        for s in symbols:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(s, ' ')\n",
        "        uncleaned_tweet = uncleaned_tweet.replace('&', ' and ')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_symbols(uncleaned_tweet)\n",
        "    \n",
        "    \n",
        "    def remove_emote(uncleaned_tweet):\n",
        "        for emote in emotes:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(emote, '')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_emote(uncleaned_tweet)\n",
        "    word_tokens = word_tokenize(uncleaned_tweet)\n",
        "\n",
        "\n",
        "    def remove_links(word_tokens):\n",
        "        if 'http' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('http')]\n",
        "        if 'https' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('https')]\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_links(word_tokens)\n",
        "\n",
        "\n",
        "    def delete_contraction(word_tokens):\n",
        "        for i, w in enumerate(word_tokens):\n",
        "            if w == \"'d\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"n't\":\n",
        "                word_tokens[i] = 'not'\n",
        "            elif w == \"'m\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'re\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == 'gon':\n",
        "                word_tokens[i] = 'going to'\n",
        "            elif w == 'na':\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'s\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'ll\":\n",
        "                word_tokens[i] = ''\n",
        "            else:\n",
        "                pass\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = delete_contraction(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_at(word_tokens):  # remove '@' and words followed by @\n",
        "        j = 0\n",
        "        for i in range(len(word_tokens)):\n",
        "            try:\n",
        "                if word_tokens[j] == '@':\n",
        "                    del word_tokens[j + 1]\n",
        "                    del word_tokens[j]\n",
        "                    j += -1\n",
        "                j += 1\n",
        "            except:\n",
        "                break\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_at(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_numbers(word_tokens):\n",
        "      j = 0\n",
        "      for i in range(len(word_tokens)):\n",
        "        try:\n",
        "          if word_tokens[i-j].isnumeric():\n",
        "            del word_tokens[i-j]\n",
        "            j += 1\n",
        "        except:\n",
        "          break\n",
        "      return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_numbers(word_tokens)\n",
        "\n",
        "    cleaned_tweet = ' '.join(word_tokens).replace('_', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('#', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('/', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace(\"'\", ' ')\n",
        "\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n",
        "def clean_tweet_dataframe(df):  # Jaki's, not used but more efficient\n",
        "  data = df.string.values\n",
        "  clean_data = []\n",
        "  ######################## Denoising Texts #########################\n",
        "  letters = {'A', 'B', 'C', 'E', 'F', 'G', 'H', 'M', 'N', 'P', 'S', 'V', 'Y', 'a', 'b', 'v'}\n",
        "  symbols = {'!', '#', '$', '&', \"'\", '(', ')', '*', '+', '-', '_', '.', '/', ':', ';', '=', '?', '`', '|', '~',\n",
        "            '[', ']', '^',\n",
        "            '\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ', '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ',\n",
        "            '\\x89ÛÒ', 'Ûªs', 'Û_'}\n",
        "  start = time.time()\n",
        "  for d in data:\n",
        "      ws = d.split(' ')\n",
        "      for i, w in enumerate(ws):\n",
        "          # process 1: remove https and @\n",
        "          if ('http' in w) or ('@' in w):\n",
        "              ws[i] = ''\n",
        "          # process 2: remove single letters\n",
        "          if w in letters:\n",
        "              ws[i] = ''\n",
        "      new_d = ' '.join(ws)\n",
        "      # process 3: remove symbols and emotes\n",
        "      for s in symbols:\n",
        "          new_d = new_d.replace(s, ' ')\n",
        "      # process 4: replace multiple spaces with one space\n",
        "      new_d = ' '.join(new_d.split())\n",
        "      clean_data.append(new_d.lower())\n",
        "  end = time.time() - start\n",
        "  print(end)\n",
        "  print(len(clean_data))\n",
        "  return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlEDnkdWd1FI",
        "colab_type": "text"
      },
      "source": [
        "## Spell-Checker and Remove_Stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwj599LheNmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spellcheck(input_words):  # Additionally casts to LC and removes excess spaces as a nice side-effect of how it spell-checks.\n",
        "  speller = SpellChecker()\n",
        "  words = speller.split_words(input_words)\n",
        "  [speller.correction(w) for w in words]\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "def remove_stopwords(input_words):\n",
        "  stop_words = set(stopwords.words('english'))  # List of stopwords to remove\n",
        "  word_tokens = word_tokenize(input_words)\n",
        "  words = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "  return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZIoaXKfIwm",
        "colab_type": "text"
      },
      "source": [
        "## Other Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "souan2Eg7QEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dim(input_tweets):\n",
        "  m = 0\n",
        "  for i in range(len(input_tweets)):\n",
        "    word_tokens = word_tokenize(input_tweets[i])\n",
        "    m = max(m, len(word_tokens))\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_usfa7gtekIK",
        "colab_type": "text"
      },
      "source": [
        "## Data Import and Pre-Processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PZNUK32oqSD",
        "colab_type": "code",
        "outputId": "ce2de650-f4c1-41cf-81da-36343c0c5c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Try to import the cleaned sheet, if it doesn't exist, re-clean the raw data\n",
        "\n",
        "try:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  data = pd.read_csv(input_file).to_numpy()\n",
        "  print(\"Cleaned data successfully read from file!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Ugly_words_FULL - Sheet1.csv\"\n",
        "  df = pd.read_csv(input_file)\n",
        "  labels = df.columns.values\n",
        "  X = df.to_numpy()\n",
        "  Y = X[:1500, 0:6].astype(int) # Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News?\n",
        "  X = X[:1500, 6].astype(str)  # Dimensionless to pass strings/elements, expand to do matrix operations\n",
        "  # print(Y.shape)\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = clean_tweet(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = spellcheck(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = remove_stopwords(X[i])\n",
        "\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  X = np.expand_dims(X, axis=1)\n",
        "  data = np.concatenate((Y, X), axis=1)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.columns = labels\n",
        "\n",
        "  output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  df.to_csv(output_file, header=True, index=False)\n",
        "\n",
        "  print(\"Cleaned data successfully written to file!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaned data successfully read from file!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlHf_xz1GB6S",
        "colab_type": "code",
        "outputId": "5d9f9688-f6ae-42c2-c5f8-ee15447b79a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Oversample if desired (probably yes).\n",
        "y_start = 1\n",
        "y_end = 2\n",
        "\n",
        "oversample = True\n",
        "num_copies = 3  # Number of copies to add\n",
        "\n",
        "if oversample:\n",
        "  minority = data[:, :][data[:, y_start] == 1]\n",
        "  print(minority.shape)\n",
        "  \n",
        "  for i in range(num_copies):\n",
        "    np.random.shuffle(minority)\n",
        "    temp = minority\n",
        "    data = np.append(data, temp[:200, :], axis=0)\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(886, 7)\n",
            "(2100, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZt_7s5DRb0T",
        "colab_type": "code",
        "outputId": "006d3ff4-5c8a-479e-cfed-261568428d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Check sizes and split into train and test sets.\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News? [0:4]\n",
        "Y_train = data[:1100, y_start:y_end].astype(int).squeeze()\n",
        "X_train = data[:1100, 6].astype(str)\n",
        "Y_valid = data[1100:1600, y_start:y_end].astype(int).squeeze()\n",
        "X_valid = data[1100:1600, 6].astype(str)\n",
        "Y_test = data[1600:2100, y_start:y_end].astype(int).squeeze()\n",
        "X_test = data[1600:2100, 6].astype(str)\n",
        "\n",
        "# Print splits.\n",
        "print(len(X_train[Y_train==1]))\n",
        "print(len(X_valid[Y_valid==1]))\n",
        "print(len(X_test[Y_test==1]))\n",
        "\n",
        "# Tokenize and standardize lengths for training:\n",
        "token_len = find_dim(data[:, 6].astype(str))\n",
        "print(\"Padding Length: \" + str(token_len))\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.fit_on_texts(X_valid)\n",
        "# tokenizer.fit_on_texts(X_test)  # Don't fit over test since it's not part of the model.\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)  # Vectorizes into numbers\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)  # Vectorizes into numbers\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)  # Vectorizes into numbers\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=token_len, padding='pre', truncating='post')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "790\n",
            "334\n",
            "362\n",
            "Padding Length: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8Xp1mIeojP",
        "colab_type": "text"
      },
      "source": [
        "## Tensorization and Embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSk44mu0j5uD",
        "colab_type": "code",
        "outputId": "550a4dc4-9fcc-4985-dc0e-73cda4d6f271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Tensorization, shuffle and batch so the format is correct for embedding.\n",
        "X_train = tf.constant(X_train, dtype=tf.int64)\n",
        "Y_train = tf.constant(Y_train, dtype=tf.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(100).batch(100)\n",
        "print(train_data)\n",
        "\n",
        "X_valid = tf.constant(X_valid, dtype=tf.int64)\n",
        "Y_valid = tf.constant(Y_valid, dtype=tf.int64)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid)).shuffle(100).batch(100)\n",
        "print(valid_data)\n",
        "\n",
        "X_test = tf.constant(X_test, dtype=tf.int64)\n",
        "Y_test = tf.constant(Y_test, dtype=tf.int64)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(100).batch(100)\n",
        "print(test_data)\n",
        "\n",
        "# print(list(test_data.as_numpy_iterator()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEXAb3fe3qA",
        "colab_type": "text"
      },
      "source": [
        "## Build and Train Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc6Y16rCgXHz",
        "colab_type": "code",
        "outputId": "a325ef7a-77b2-467e-8dbe-c73d02a63c6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Define model layers with Dense over LSTM for Classification\n",
        "print(len(tokenizer.word_index))\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 1000))\n",
        "# Flatten to 1D to line up with subsequent layers since Embedding can take multiple and Dense cannot\n",
        "model.add(tf.keras.layers.LSTM(100, activation=\"relu\", return_sequences=False, recurrent_dropout=0.1))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4953\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 1000)        4954000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               440400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 5,424,801\n",
            "Trainable params: 5,424,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9nUvMtjijR",
        "colab_type": "code",
        "outputId": "ca55b244-7e73-4244-9be8-1fc1d5f34df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define model parameters after defining layers with compile:\n",
        "tf.keras.backend.clear_session()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Run the fitting and train the model:\n",
        "# batch_size = 100 # Number of observations run per set\n",
        "history = model.fit(train_data, validation_data=valid_data, epochs=100, verbose=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 2s 182ms/step - loss: 0.6570 - accuracy: 0.6845 - val_loss: 0.6896 - val_accuracy: 0.6680\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.5668 - accuracy: 0.7182 - val_loss: 0.5934 - val_accuracy: 0.6680\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 0.3773 - accuracy: 0.7182 - val_loss: 0.5566 - val_accuracy: 0.6680\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.2415 - accuracy: 0.7391 - val_loss: 0.6991 - val_accuracy: 0.7020\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.1793 - accuracy: 0.9927 - val_loss: 1.1011 - val_accuracy: 0.7660\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 6.8952 - accuracy: 0.9391 - val_loss: 0.9390 - val_accuracy: 0.7700\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 561.2062 - accuracy: 0.9718 - val_loss: 0.5212 - val_accuracy: 0.7460\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.1495 - accuracy: 0.9400 - val_loss: 0.4266 - val_accuracy: 0.7740\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0645 - accuracy: 0.9882 - val_loss: 0.4471 - val_accuracy: 0.7900\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0296 - accuracy: 0.9955 - val_loss: 0.4811 - val_accuracy: 0.7580\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0183 - accuracy: 0.9945 - val_loss: 0.5212 - val_accuracy: 0.7620\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0122 - accuracy: 0.9955 - val_loss: 0.5587 - val_accuracy: 0.7600\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0105 - accuracy: 0.9964 - val_loss: 0.5907 - val_accuracy: 0.7720\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0084 - accuracy: 0.9955 - val_loss: 0.6171 - val_accuracy: 0.7680\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0073 - accuracy: 0.9964 - val_loss: 0.6397 - val_accuracy: 0.7700\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0076 - accuracy: 0.9955 - val_loss: 0.6599 - val_accuracy: 0.7720\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0081 - accuracy: 0.9955 - val_loss: 0.6752 - val_accuracy: 0.7680\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0075 - accuracy: 0.9955 - val_loss: 0.6890 - val_accuracy: 0.7680\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0069 - accuracy: 0.9945 - val_loss: 0.7004 - val_accuracy: 0.7700\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0050 - accuracy: 0.9973 - val_loss: 0.7120 - val_accuracy: 0.7700\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0067 - accuracy: 0.9964 - val_loss: 0.7212 - val_accuracy: 0.7700\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0058 - accuracy: 0.9973 - val_loss: 0.7292 - val_accuracy: 0.7700\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0048 - accuracy: 0.9973 - val_loss: 0.7384 - val_accuracy: 0.7700\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0059 - accuracy: 0.9955 - val_loss: 0.7469 - val_accuracy: 0.7720\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0060 - accuracy: 0.9964 - val_loss: 0.7531 - val_accuracy: 0.7720\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 0.0067 - accuracy: 0.9955 - val_loss: 0.7608 - val_accuracy: 0.7640\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0071 - accuracy: 0.9964 - val_loss: 0.7648 - val_accuracy: 0.7720\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0055 - accuracy: 0.9955 - val_loss: 0.7717 - val_accuracy: 0.7640\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0056 - accuracy: 0.9973 - val_loss: 0.7767 - val_accuracy: 0.7680\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0056 - accuracy: 0.9973 - val_loss: 0.7831 - val_accuracy: 0.7680\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0071 - accuracy: 0.9964 - val_loss: 0.7871 - val_accuracy: 0.7700\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0060 - accuracy: 0.9955 - val_loss: 0.7899 - val_accuracy: 0.7720\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0055 - accuracy: 0.9973 - val_loss: 0.7947 - val_accuracy: 0.7720\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0061 - accuracy: 0.9955 - val_loss: 0.7986 - val_accuracy: 0.7680\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 0.0047 - accuracy: 0.9973 - val_loss: 0.8028 - val_accuracy: 0.7680\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0052 - accuracy: 0.9964 - val_loss: 0.8079 - val_accuracy: 0.7720\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.8131 - val_accuracy: 0.7740\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.8204 - val_accuracy: 0.7700\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0056 - accuracy: 0.9964 - val_loss: 0.8270 - val_accuracy: 0.7700\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 0.8327 - val_accuracy: 0.7720\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 0.0055 - accuracy: 0.9955 - val_loss: 0.8380 - val_accuracy: 0.7720\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0050 - accuracy: 0.9973 - val_loss: 0.8408 - val_accuracy: 0.7700\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0050 - accuracy: 0.9973 - val_loss: 0.8462 - val_accuracy: 0.7720\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 0.0049 - accuracy: 0.9964 - val_loss: 0.8516 - val_accuracy: 0.7720\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0050 - accuracy: 0.9955 - val_loss: 0.8560 - val_accuracy: 0.7720\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0044 - accuracy: 0.9973 - val_loss: 0.8610 - val_accuracy: 0.7720\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0053 - accuracy: 0.9964 - val_loss: 0.8685 - val_accuracy: 0.7700\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 0.0053 - accuracy: 0.9955 - val_loss: 0.8754 - val_accuracy: 0.7720\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 0.0042 - accuracy: 0.9982 - val_loss: 0.8817 - val_accuracy: 0.7700\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 0.0045 - accuracy: 0.9973 - val_loss: 0.8891 - val_accuracy: 0.7700\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 0.0044 - accuracy: 0.9964 - val_loss: 0.8963 - val_accuracy: 0.7740\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0049 - accuracy: 0.9945 - val_loss: 0.9046 - val_accuracy: 0.7720\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 0.9119 - val_accuracy: 0.7720\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0042 - accuracy: 0.9973 - val_loss: 0.9195 - val_accuracy: 0.7720\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 0.0045 - accuracy: 0.9973 - val_loss: 0.9306 - val_accuracy: 0.7720\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.9379 - val_accuracy: 0.7720\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0049 - accuracy: 0.9955 - val_loss: 0.9425 - val_accuracy: 0.7700\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 0.0041 - accuracy: 0.9973 - val_loss: 0.9488 - val_accuracy: 0.7720\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0043 - accuracy: 0.9964 - val_loss: 0.9540 - val_accuracy: 0.7700\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0042 - accuracy: 0.9973 - val_loss: 0.9586 - val_accuracy: 0.7700\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 0.0039 - accuracy: 0.9973 - val_loss: 0.9663 - val_accuracy: 0.7700\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 0.0043 - accuracy: 0.9973 - val_loss: 0.9712 - val_accuracy: 0.7700\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0041 - accuracy: 0.9973 - val_loss: 0.9764 - val_accuracy: 0.7700\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0040 - accuracy: 0.9982 - val_loss: 0.9834 - val_accuracy: 0.7720\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 0.0044 - accuracy: 0.9964 - val_loss: 0.9883 - val_accuracy: 0.7720\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0044 - accuracy: 0.9973 - val_loss: 0.9913 - val_accuracy: 0.7720\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0043 - accuracy: 0.9964 - val_loss: 0.9958 - val_accuracy: 0.7700\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 0.0041 - accuracy: 0.9973 - val_loss: 0.9974 - val_accuracy: 0.7680\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 0.0038 - accuracy: 0.9973 - val_loss: 0.9988 - val_accuracy: 0.7720\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 0.0040 - accuracy: 0.9964 - val_loss: 1.0033 - val_accuracy: 0.7720\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0043 - accuracy: 0.9964 - val_loss: 1.0058 - val_accuracy: 0.7720\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0039 - accuracy: 0.9973 - val_loss: 1.0102 - val_accuracy: 0.7700\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 0.0039 - accuracy: 0.9973 - val_loss: 1.0137 - val_accuracy: 0.7700\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0039 - accuracy: 0.9982 - val_loss: 1.0131 - val_accuracy: 0.7740\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0041 - accuracy: 0.9964 - val_loss: 1.0159 - val_accuracy: 0.7740\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0039 - accuracy: 0.9973 - val_loss: 1.0160 - val_accuracy: 0.7720\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0038 - accuracy: 0.9973 - val_loss: 1.0230 - val_accuracy: 0.7740\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 0.0040 - accuracy: 0.9982 - val_loss: 1.0271 - val_accuracy: 0.7700\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0055 - accuracy: 0.9964 - val_loss: 1.0255 - val_accuracy: 0.7740\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0038 - accuracy: 0.9973 - val_loss: 1.0221 - val_accuracy: 0.7740\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0042 - accuracy: 0.9964 - val_loss: 1.0228 - val_accuracy: 0.7720\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0038 - accuracy: 0.9973 - val_loss: 1.0265 - val_accuracy: 0.7740\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 0.0040 - accuracy: 0.9964 - val_loss: 1.0276 - val_accuracy: 0.7700\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0041 - accuracy: 0.9973 - val_loss: 1.0309 - val_accuracy: 0.7720\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0036 - accuracy: 0.9982 - val_loss: 1.0327 - val_accuracy: 0.7760\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0037 - accuracy: 0.9973 - val_loss: 1.0343 - val_accuracy: 0.7700\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.0371 - val_accuracy: 0.7700\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0042 - accuracy: 0.9973 - val_loss: 1.0405 - val_accuracy: 0.7680\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 0.0042 - accuracy: 0.9982 - val_loss: 1.0422 - val_accuracy: 0.7680\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0037 - accuracy: 0.9973 - val_loss: 1.0455 - val_accuracy: 0.7720\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0041 - accuracy: 0.9964 - val_loss: 1.0498 - val_accuracy: 0.7680\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0037 - accuracy: 0.9982 - val_loss: 1.0525 - val_accuracy: 0.7740\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 0.0040 - accuracy: 0.9945 - val_loss: 1.0553 - val_accuracy: 0.7720\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 0.0038 - accuracy: 0.9982 - val_loss: 1.0567 - val_accuracy: 0.7740\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0043 - accuracy: 0.9964 - val_loss: 1.0585 - val_accuracy: 0.7700\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0038 - accuracy: 0.9964 - val_loss: 1.0608 - val_accuracy: 0.7660\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 0.0038 - accuracy: 0.9973 - val_loss: 1.0623 - val_accuracy: 0.7720\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0037 - accuracy: 0.9973 - val_loss: 1.0640 - val_accuracy: 0.7700\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0038 - accuracy: 0.9973 - val_loss: 1.0654 - val_accuracy: 0.7700\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0038 - accuracy: 0.9964 - val_loss: 1.0685 - val_accuracy: 0.7720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS0CEZ1me5S4",
        "colab_type": "text"
      },
      "source": [
        "## Predict and Metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z95XMfjtiPP0",
        "colab_type": "code",
        "outputId": "51b4f71a-be05-4402-edf7-9804c9703289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model.evaluate(test_data, verbose=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 - 0s - loss: 1.2967 - accuracy: 0.8100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZo1fqLiLj-",
        "colab_type": "code",
        "outputId": "8fe80d68-9e19-4a0b-9a6d-1af26b079d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 1.297\n",
            "accuracy: 0.810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSs6J5Aiyf3",
        "colab_type": "code",
        "outputId": "2f77833d-f567-41fb-da58-d7bfaf9d5d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "test_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\n",
        "test_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\n",
        "test_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase1)))\n",
        "\n",
        "test_phrase2 = \"urgent news from israel\"\n",
        "test_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\n",
        "test_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase2)))\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\\ntest_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\\ntest_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase1)))\\n\\ntest_phrase2 = \"urgent news from israel\"\\ntest_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\\ntest_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase2)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLpc0QLRxQ4q",
        "colab_type": "code",
        "outputId": "668a2da9-df17-47d0-fe05-70eea551e093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "Y_hat = model.predict_classes(X_test).reshape(len(Y_test), 1)\n",
        "print(len(Y_hat))\n",
        "print(type(Y_hat))\n",
        "print(len(Y_hat[Y_hat==1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-be29c51753dd>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "500\n",
            "<class 'numpy.ndarray'>\n",
            "367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4uHbgPD7eGD",
        "colab_type": "code",
        "outputId": "9fd68473-0c02-4d6a-8ca1-28e7b43d5d88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Make Confusion Matrix\n",
        "plt.figure(figsize=(9, 6))\n",
        "y_true = pd.Series(Y_test.numpy().squeeze(), name=\"Actual Label\")\n",
        "y_pred = pd.Series(Y_hat.squeeze(), name=\"Predicted Label\")\n",
        "sns.heatmap(pd.crosstab(y_true, y_pred), annot=True, fmt=\"d\", linewidths=0.25)\n",
        "plt.ylim(len(set(Y_test.numpy())), 0)  # Fix limits, matplotlib bugged (ver. 3.11)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.0, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAFzCAYAAABM02E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wGiVof2iPhuDgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ8ktCDu6tWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = np.expand_dims(X, axis=1)\n",
        "output = np.concatenate((Y_hat, Y_test.numpy().reshape(-1, 1), data[1600:2100, 6:7].reshape(-1, 1)), axis=1)\n",
        "df = pd.DataFrame(output)\n",
        "\n",
        "output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/NLP_Tweet_Classification_Results.csv\"\n",
        "df.to_csv(output_file, header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLNQXNktgyf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}