{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Tweet Classification_0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5a2fi7JePkG",
        "colab_type": "text"
      },
      "source": [
        "## Import Neccessary Packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyilNou3d0Ax",
        "colab_type": "code",
        "outputId": "ca526fa0-6573-4b72-ba62-226d51ade092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  # Returns split words, while tf vectorizes after splitting.\n",
        "\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xAKlUqfqgn",
        "colab_type": "code",
        "outputId": "88b504d4-2f8e-4c8d-decb-2d49c7366acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (46.0.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (2.1.0.dev202003290105)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.21.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.38.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.21.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly) (46.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\n",
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8CjWN0FdTax",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQc0rSfb94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(uncleaned_tweet):\n",
        "    # Order of operations:\n",
        "    # 1. Remove symbols/emojis.\n",
        "    # 2. Tokenize for subsequent processes.\n",
        "    # 3. Remove http links.\n",
        "    # 4. Replace contractions with root words.\n",
        "    # 5. Remove @whatever.\n",
        "    # 6. Separate and Remove '_', '#', '/', ''' in that order.\n",
        "    # 7. Join and return.\n",
        "\n",
        "    # Identify junk to be removed.\n",
        "    symbols = ['*', '+', '...', '?', '`', '``', '[', ']', '(', ')', '-', '~', '|']\n",
        "    \n",
        "    emotes = ['\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ',\n",
        "              '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ', '\\x89ÛÒ', 'Ûªs']    \n",
        "    \n",
        "\n",
        "    def remove_symbols(uncleaned_tweet):\n",
        "        for s in symbols:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(s, ' ')\n",
        "        uncleaned_tweet = uncleaned_tweet.replace('&', ' and ')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_symbols(uncleaned_tweet)\n",
        "    \n",
        "    \n",
        "    def remove_emote(uncleaned_tweet):\n",
        "        for emote in emotes:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(emote, '')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_emote(uncleaned_tweet)\n",
        "    word_tokens = word_tokenize(uncleaned_tweet)\n",
        "\n",
        "\n",
        "    def remove_links(word_tokens):\n",
        "        if 'http' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('http')]\n",
        "        if 'https' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('https')]\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_links(word_tokens)\n",
        "\n",
        "\n",
        "    def delete_contraction(word_tokens):\n",
        "        for i, w in enumerate(word_tokens):\n",
        "            if w == \"'d\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"n't\":\n",
        "                word_tokens[i] = 'not'\n",
        "            elif w == \"'m\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'re\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == 'gon':\n",
        "                word_tokens[i] = 'going to'\n",
        "            elif w == 'na':\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'s\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'ll\":\n",
        "                word_tokens[i] = ''\n",
        "            else:\n",
        "                pass\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = delete_contraction(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_at(word_tokens):  # remove '@' and words followed by @\n",
        "        j = 0\n",
        "        for i in range(len(word_tokens)):\n",
        "            try:\n",
        "                if word_tokens[j] == '@':\n",
        "                    del word_tokens[j + 1]\n",
        "                    del word_tokens[j]\n",
        "                    j += -1\n",
        "                j += 1\n",
        "            except:\n",
        "                break\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_at(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_numbers(word_tokens):\n",
        "      j = 0\n",
        "      for i in range(len(word_tokens)):\n",
        "        try:\n",
        "          if word_tokens[i-j].isnumeric():\n",
        "            del word_tokens[i-j]\n",
        "            j += 1\n",
        "        except:\n",
        "          break\n",
        "      return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_numbers(word_tokens)\n",
        "\n",
        "    cleaned_tweet = ' '.join(word_tokens).replace('_', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('#', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('/', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace(\"'\", ' ')\n",
        "\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n",
        "def clean_tweet_dataframe(df):  # Jaki's, not used but more efficient\n",
        "  data = df.string.values\n",
        "  clean_data = []\n",
        "  ######################## Denoising Texts #########################\n",
        "  letters = {'A', 'B', 'C', 'E', 'F', 'G', 'H', 'M', 'N', 'P', 'S', 'V', 'Y', 'a', 'b', 'v'}\n",
        "  symbols = {'!', '#', '$', '&', \"'\", '(', ')', '*', '+', '-', '_', '.', '/', ':', ';', '=', '?', '`', '|', '~',\n",
        "            '[', ']', '^',\n",
        "            '\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ', '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ',\n",
        "            '\\x89ÛÒ', 'Ûªs', 'Û_'}\n",
        "  start = time.time()\n",
        "  for d in data:\n",
        "      ws = d.split(' ')\n",
        "      for i, w in enumerate(ws):\n",
        "          # process 1: remove https and @\n",
        "          if ('http' in w) or ('@' in w):\n",
        "              ws[i] = ''\n",
        "          # process 2: remove single letters\n",
        "          if w in letters:\n",
        "              ws[i] = ''\n",
        "      new_d = ' '.join(ws)\n",
        "      # process 3: remove symbols and emotes\n",
        "      for s in symbols:\n",
        "          new_d = new_d.replace(s, ' ')\n",
        "      # process 4: replace multiple spaces with one space\n",
        "      new_d = ' '.join(new_d.split())\n",
        "      clean_data.append(new_d.lower())\n",
        "  end = time.time() - start\n",
        "  print(end)\n",
        "  print(len(clean_data))\n",
        "  return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlEDnkdWd1FI",
        "colab_type": "text"
      },
      "source": [
        "## Spell-Checker and Remove_Stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwj599LheNmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spellcheck(input_words):  # Additionally casts to LC and removes excess spaces as a nice side-effect of how it spell-checks.\n",
        "  speller = SpellChecker()\n",
        "  words = speller.split_words(input_words)\n",
        "  [speller.correction(w) for w in words]\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "def remove_stopwords(input_words):\n",
        "  stop_words = set(stopwords.words('english'))  # List of stopwords to remove\n",
        "  word_tokens = word_tokenize(input_words)\n",
        "  words = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "  return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZIoaXKfIwm",
        "colab_type": "text"
      },
      "source": [
        "## Other Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "souan2Eg7QEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dim(input_tweets):\n",
        "  m = 0\n",
        "  for i in range(len(input_tweets)):\n",
        "    word_tokens = word_tokenize(input_tweets[i])\n",
        "    m = max(m, len(word_tokens))\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_usfa7gtekIK",
        "colab_type": "text"
      },
      "source": [
        "## Data Import and Pre-Processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PZNUK32oqSD",
        "colab_type": "code",
        "outputId": "f10af3a9-2fcf-448b-cf0a-68258437d76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Try to import the cleaned sheet, if it doesn't exist, re-clean the raw data\n",
        "\n",
        "try:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  data = pd.read_csv(input_file).to_numpy()\n",
        "  print(\"Cleaned data successfully read from file!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Ugly_words_FULL - Sheet1.csv\"\n",
        "  df = pd.read_csv(input_file)\n",
        "  labels = df.columns.values\n",
        "  X = df.to_numpy()\n",
        "  Y = X[:1500, 0:6].astype(int) # Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News?\n",
        "  X = X[:1500, 6].astype(str)  # Dimensionless to pass strings/elements, expand to do matrix operations\n",
        "  # print(Y.shape)\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = clean_tweet(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = spellcheck(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = remove_stopwords(X[i])\n",
        "\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  X = np.expand_dims(X, axis=1)\n",
        "  data = np.concatenate((Y, X), axis=1)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.columns = labels\n",
        "\n",
        "  output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  df.to_csv(output_file, header=True, index=False)\n",
        "\n",
        "  print(\"Cleaned data successfully written to file!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaned data successfully read from file!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlHf_xz1GB6S",
        "colab_type": "code",
        "outputId": "68b9505c-c734-4416-e234-7d3d658f8c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Oversample if desired (probably yes).\n",
        "y_start = 0\n",
        "y_end = 1\n",
        "\n",
        "oversample = True\n",
        "num_copies = 3  # Number of copies to add\n",
        "\n",
        "if oversample:\n",
        "  minority = data[:, :][data[:, y_start] == 1]\n",
        "  print(minority.shape)\n",
        "  \n",
        "  for i in range(num_copies):\n",
        "    np.random.shuffle(minority)\n",
        "    temp = minority\n",
        "    data = np.append(data, temp[:200, :], axis=0)\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(357, 7)\n",
            "(2100, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZt_7s5DRb0T",
        "colab_type": "code",
        "outputId": "3165971d-cc3b-40df-a345-bcf8a000a2d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Check sizes and split into train and test sets.\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News? [0:4]\n",
        "Y_train = data[:1100, y_start:y_end].astype(int).squeeze()\n",
        "X_train = data[:1100, 6].astype(str)\n",
        "Y_valid = data[1100:1600, y_start:y_end].astype(int).squeeze()\n",
        "X_valid = data[1100:1600, 6].astype(str)\n",
        "Y_test = data[1600:2100, y_start:y_end].astype(int).squeeze()\n",
        "X_test = data[1600:2100, 6].astype(str)\n",
        "\n",
        "# Print splits.\n",
        "print(len(X_train[Y_train==1]))\n",
        "print(len(X_valid[Y_valid==1]))\n",
        "print(len(X_test[Y_test==1]))\n",
        "\n",
        "# Tokenize and standardize lengths for training:\n",
        "token_len = find_dim(data[:, 6].astype(str))\n",
        "print(\"Padding Length: \" + str(token_len))\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.fit_on_texts(X_valid)\n",
        "# tokenizer.fit_on_texts(X_test)  # Don't fit over test since it's not part of the model.\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)  # Vectorizes into numbers\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)  # Vectorizes into numbers\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)  # Vectorizes into numbers\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=token_len, padding='pre', truncating='post')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "513\n",
            "224\n",
            "220\n",
            "Padding Length: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8Xp1mIeojP",
        "colab_type": "text"
      },
      "source": [
        "## Tensorization and Embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSk44mu0j5uD",
        "colab_type": "code",
        "outputId": "6e012312-06a4-4d12-8e20-eb31714d88fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Tensorization, shuffle and batch so the format is correct for embedding.\n",
        "X_train = tf.constant(X_train, dtype=tf.int64)\n",
        "Y_train = tf.constant(Y_train, dtype=tf.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(100).batch(100)\n",
        "print(train_data)\n",
        "\n",
        "X_valid = tf.constant(X_valid, dtype=tf.int64)\n",
        "Y_valid = tf.constant(Y_valid, dtype=tf.int64)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid)).shuffle(100).batch(100)\n",
        "print(valid_data)\n",
        "\n",
        "X_test = tf.constant(X_test, dtype=tf.int64)\n",
        "Y_test = tf.constant(Y_test, dtype=tf.int64)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(100).batch(100)\n",
        "print(test_data)\n",
        "\n",
        "# print(list(test_data.as_numpy_iterator()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEXAb3fe3qA",
        "colab_type": "text"
      },
      "source": [
        "## Build and Train Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc6Y16rCgXHz",
        "colab_type": "code",
        "outputId": "76f8629a-238b-41f8-cb88-9d86aa9b8e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Define model layers with Dense over LSTM for Classification\n",
        "print(len(tokenizer.word_index))\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 1000))\n",
        "# Flatten to 1D to line up with subsequent layers since Embedding can take multiple and Dense cannot\n",
        "model.add(tf.keras.layers.LSTM(100, activation=\"relu\", return_sequences=False, recurrent_dropout=0.1))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4954\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 1000)        4955000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               440400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 5,425,801\n",
            "Trainable params: 5,425,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9nUvMtjijR",
        "colab_type": "code",
        "outputId": "4e524ea0-bfee-4ac4-a9e6-4e8c752323c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define model parameters after defining layers with compile:\n",
        "tf.keras.backend.clear_session()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Run the fitting and train the model:\n",
        "# batch_size = 100 # Number of observations run per set\n",
        "history = model.fit(train_data, validation_data=valid_data, epochs=100, verbose=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 2s 182ms/step - loss: 0.6910 - accuracy: 0.5100 - val_loss: 0.6803 - val_accuracy: 0.5520\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.6215 - accuracy: 0.5891 - val_loss: 0.5459 - val_accuracy: 0.7100\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.2622 - accuracy: 0.9327 - val_loss: 0.3788 - val_accuracy: 0.8660\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0631 - accuracy: 0.9900 - val_loss: 0.7900 - val_accuracy: 0.7240\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0211 - accuracy: 0.9936 - val_loss: 0.5323 - val_accuracy: 0.8420\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.5666 - val_accuracy: 0.8440\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0075 - accuracy: 0.9955 - val_loss: 0.6279 - val_accuracy: 0.8340\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0057 - accuracy: 0.9964 - val_loss: 0.6644 - val_accuracy: 0.8340\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0062 - accuracy: 0.9955 - val_loss: 0.6953 - val_accuracy: 0.8360\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0056 - accuracy: 0.9964 - val_loss: 0.6980 - val_accuracy: 0.8260\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0055 - accuracy: 0.9955 - val_loss: 0.7180 - val_accuracy: 0.8320\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0055 - accuracy: 0.9964 - val_loss: 0.7277 - val_accuracy: 0.8320\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0054 - accuracy: 0.9964 - val_loss: 0.7430 - val_accuracy: 0.8240\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0058 - accuracy: 0.9955 - val_loss: 0.7387 - val_accuracy: 0.8260\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0053 - accuracy: 0.9964 - val_loss: 0.7388 - val_accuracy: 0.8260\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0052 - accuracy: 0.9964 - val_loss: 0.7570 - val_accuracy: 0.8160\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0057 - accuracy: 0.9955 - val_loss: 0.7636 - val_accuracy: 0.8220\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0052 - accuracy: 0.9964 - val_loss: 0.7762 - val_accuracy: 0.8220\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0052 - accuracy: 0.9964 - val_loss: 0.7927 - val_accuracy: 0.8200\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0049 - accuracy: 0.9955 - val_loss: 0.8304 - val_accuracy: 0.8200\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0054 - accuracy: 0.9955 - val_loss: 0.8283 - val_accuracy: 0.8180\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0052 - accuracy: 0.9955 - val_loss: 0.8385 - val_accuracy: 0.8200\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0053 - accuracy: 0.9955 - val_loss: 0.8182 - val_accuracy: 0.8240\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0052 - accuracy: 0.9955 - val_loss: 0.8349 - val_accuracy: 0.8200\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0057 - accuracy: 0.9955 - val_loss: 0.8323 - val_accuracy: 0.8200\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0051 - accuracy: 0.9964 - val_loss: 0.8085 - val_accuracy: 0.8200\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0051 - accuracy: 0.9955 - val_loss: 0.8110 - val_accuracy: 0.8220\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0050 - accuracy: 0.9964 - val_loss: 0.8401 - val_accuracy: 0.8200\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0050 - accuracy: 0.9955 - val_loss: 0.8440 - val_accuracy: 0.8180\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0049 - accuracy: 0.9973 - val_loss: 0.8495 - val_accuracy: 0.8160\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0051 - accuracy: 0.9955 - val_loss: 0.8649 - val_accuracy: 0.8160\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0050 - accuracy: 0.9955 - val_loss: 0.8883 - val_accuracy: 0.8160\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0049 - accuracy: 0.9973 - val_loss: 0.8998 - val_accuracy: 0.8160\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0051 - accuracy: 0.9955 - val_loss: 0.9015 - val_accuracy: 0.8100\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0048 - accuracy: 0.9964 - val_loss: 0.9172 - val_accuracy: 0.8120\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0051 - accuracy: 0.9955 - val_loss: 0.9202 - val_accuracy: 0.8140\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0051 - accuracy: 0.9964 - val_loss: 0.9088 - val_accuracy: 0.8160\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0051 - accuracy: 0.9955 - val_loss: 0.8934 - val_accuracy: 0.8160\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0051 - accuracy: 0.9955 - val_loss: 0.9013 - val_accuracy: 0.8140\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0050 - accuracy: 0.9955 - val_loss: 0.9079 - val_accuracy: 0.8140\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0050 - accuracy: 0.9964 - val_loss: 0.9063 - val_accuracy: 0.8140\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0051 - accuracy: 0.9964 - val_loss: 0.9229 - val_accuracy: 0.8140\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0049 - accuracy: 0.9973 - val_loss: 0.9109 - val_accuracy: 0.8180\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0049 - accuracy: 0.9955 - val_loss: 0.9223 - val_accuracy: 0.8160\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0048 - accuracy: 0.9964 - val_loss: 0.9278 - val_accuracy: 0.8180\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0050 - accuracy: 0.9955 - val_loss: 0.9402 - val_accuracy: 0.8160\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0050 - accuracy: 0.9955 - val_loss: 0.9401 - val_accuracy: 0.8200\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0049 - accuracy: 0.9964 - val_loss: 0.9318 - val_accuracy: 0.8200\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0049 - accuracy: 0.9955 - val_loss: 0.9503 - val_accuracy: 0.8180\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 0.9316 - val_accuracy: 0.8260\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0049 - accuracy: 0.9955 - val_loss: 0.9312 - val_accuracy: 0.8280\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 0.9362 - val_accuracy: 0.8260\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 0.9523 - val_accuracy: 0.8280\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0047 - accuracy: 0.9973 - val_loss: 0.9614 - val_accuracy: 0.8260\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 0.9686 - val_accuracy: 0.8260\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 0.9906 - val_accuracy: 0.8240\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0049 - accuracy: 0.9955 - val_loss: 1.0028 - val_accuracy: 0.8160\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0047 - accuracy: 0.9973 - val_loss: 1.0011 - val_accuracy: 0.8280\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.0012 - val_accuracy: 0.8240\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 0.9987 - val_accuracy: 0.8240\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0049 - accuracy: 0.9955 - val_loss: 1.0095 - val_accuracy: 0.8260\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0046 - accuracy: 0.9964 - val_loss: 1.0224 - val_accuracy: 0.8280\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.0368 - val_accuracy: 0.8260\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0049 - accuracy: 0.9955 - val_loss: 1.0427 - val_accuracy: 0.8260\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0048 - accuracy: 0.9964 - val_loss: 1.0295 - val_accuracy: 0.8240\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.0351 - val_accuracy: 0.8240\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.0420 - val_accuracy: 0.8280\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.0496 - val_accuracy: 0.8260\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.0564 - val_accuracy: 0.8280\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0048 - accuracy: 0.9964 - val_loss: 1.0574 - val_accuracy: 0.8280\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.0637 - val_accuracy: 0.8260\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0048 - accuracy: 0.9964 - val_loss: 1.0610 - val_accuracy: 0.8260\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0048 - accuracy: 0.9973 - val_loss: 1.0654 - val_accuracy: 0.8240\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0046 - accuracy: 0.9973 - val_loss: 1.0633 - val_accuracy: 0.8280\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.0613 - val_accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.0623 - val_accuracy: 0.8240\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.0650 - val_accuracy: 0.8240\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.0732 - val_accuracy: 0.8280\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.0854 - val_accuracy: 0.8240\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.0879 - val_accuracy: 0.8240\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.0911 - val_accuracy: 0.8240\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.0876 - val_accuracy: 0.8280\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.0923 - val_accuracy: 0.8260\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.0988 - val_accuracy: 0.8240\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.1083 - val_accuracy: 0.8240\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.1155 - val_accuracy: 0.8160\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0048 - accuracy: 0.9964 - val_loss: 1.1309 - val_accuracy: 0.8140\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.1312 - val_accuracy: 0.8140\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0047 - accuracy: 0.9973 - val_loss: 1.1382 - val_accuracy: 0.8140\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.1209 - val_accuracy: 0.8220\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.1233 - val_accuracy: 0.8220\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.1244 - val_accuracy: 0.8180\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.1386 - val_accuracy: 0.8140\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.1288 - val_accuracy: 0.8180\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.1306 - val_accuracy: 0.8220\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 0.0047 - accuracy: 0.9964 - val_loss: 1.1291 - val_accuracy: 0.8180\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.1467 - val_accuracy: 0.8220\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.1429 - val_accuracy: 0.8160\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0048 - accuracy: 0.9955 - val_loss: 1.1563 - val_accuracy: 0.8140\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0047 - accuracy: 0.9955 - val_loss: 1.1536 - val_accuracy: 0.8220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS0CEZ1me5S4",
        "colab_type": "text"
      },
      "source": [
        "## Predict and Metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z95XMfjtiPP0",
        "colab_type": "code",
        "outputId": "f6bf5fb9-bc55-4967-d6b6-dd3c85c42996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model.evaluate(test_data, verbose=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 - 0s - loss: 1.1163 - accuracy: 0.8640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZo1fqLiLj-",
        "colab_type": "code",
        "outputId": "c03448f6-9c61-4ddf-a4aa-4e5d8b91d214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 1.116\n",
            "accuracy: 0.864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSs6J5Aiyf3",
        "colab_type": "code",
        "outputId": "5ceddd4c-7a94-4929-efe0-35caed49caed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "test_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\n",
        "test_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\n",
        "test_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase1)))\n",
        "\n",
        "test_phrase2 = \"urgent news from israel\"\n",
        "test_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\n",
        "test_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase2)))\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\\ntest_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\\ntest_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase1)))\\n\\ntest_phrase2 = \"urgent news from israel\"\\ntest_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\\ntest_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase2)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLpc0QLRxQ4q",
        "colab_type": "code",
        "outputId": "135b3633-3a9c-4181-a97a-ec1eb7abe7b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "Y_hat = model.predict_classes(X_test).reshape(len(Y_test), 1)\n",
        "print(len(Y_hat))\n",
        "print(type(Y_hat))\n",
        "print(len(Y_hat[Y_hat==1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-be29c51753dd>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "500\n",
            "<class 'numpy.ndarray'>\n",
            "252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4uHbgPD7eGD",
        "colab_type": "code",
        "outputId": "46602d29-2e8d-4b67-a7a8-fe1479239dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Make Confusion Matrix\n",
        "plt.figure(figsize=(9, 6))\n",
        "y_true = pd.Series(Y_test.numpy().squeeze(), name=\"Actual Label\")\n",
        "y_pred = pd.Series(Y_hat.squeeze(), name=\"Predicted Label\")\n",
        "sns.heatmap(pd.crosstab(y_true, y_pred), annot=True, fmt=\"d\", linewidths=0.25)\n",
        "plt.ylim(len(set(Y_test.numpy())), 0)  # Fix limits, matplotlib bugged (ver. 3.11)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.0, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAFzCAYAAABM02E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ8ktCDu6tWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = np.expand_dims(X, axis=1)\n",
        "output = np.concatenate((Y_hat, Y_test.numpy().reshape(-1, 1), data[1600:2100, 6:7].reshape(-1, 1)), axis=1)\n",
        "df = pd.DataFrame(output)\n",
        "\n",
        "output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/NLP_Tweet_Classification_Results.csv\"\n",
        "df.to_csv(output_file, header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLNQXNktgyf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}