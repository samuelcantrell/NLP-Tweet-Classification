{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Tweet Classification_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5a2fi7JePkG",
        "colab_type": "text"
      },
      "source": [
        "## Import Neccessary Packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyilNou3d0Ax",
        "colab_type": "code",
        "outputId": "4ac350c6-3df7-40ac-fd5b-73a8c76231b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  # Returns split words, while tf vectorizes after splitting.\n",
        "\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xAKlUqfqgn",
        "colab_type": "code",
        "outputId": "5fb7b02d-7433-4d89-a609-21b25dd00687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (46.0.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (2.1.0.dev202003290105)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.38.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.21.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.21.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.8)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly) (46.0.0)\n",
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8CjWN0FdTax",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQc0rSfb94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(uncleaned_tweet):\n",
        "    # Order of operations:\n",
        "    # 1. Remove symbols/emojis.\n",
        "    # 2. Tokenize for subsequent processes.\n",
        "    # 3. Remove http links.\n",
        "    # 4. Replace contractions with root words.\n",
        "    # 5. Remove @whatever.\n",
        "    # 6. Separate and Remove '_', '#', '/', ''' in that order.\n",
        "    # 7. Join and return.\n",
        "\n",
        "    # Identify junk to be removed.\n",
        "    symbols = ['*', '+', '...', '?', '`', '``', '[', ']', '(', ')', '-', '~', '|']\n",
        "    \n",
        "    emotes = ['\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ',\n",
        "              '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ', '\\x89ÛÒ', 'Ûªs']    \n",
        "    \n",
        "\n",
        "    def remove_symbols(uncleaned_tweet):\n",
        "        for s in symbols:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(s, ' ')\n",
        "        uncleaned_tweet = uncleaned_tweet.replace('&', ' and ')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_symbols(uncleaned_tweet)\n",
        "    \n",
        "    \n",
        "    def remove_emote(uncleaned_tweet):\n",
        "        for emote in emotes:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(emote, '')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_emote(uncleaned_tweet)\n",
        "    word_tokens = word_tokenize(uncleaned_tweet)\n",
        "\n",
        "\n",
        "    def remove_links(word_tokens):\n",
        "        if 'http' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('http')]\n",
        "        if 'https' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('https')]\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_links(word_tokens)\n",
        "\n",
        "\n",
        "    def delete_contraction(word_tokens):\n",
        "        for i, w in enumerate(word_tokens):\n",
        "            if w == \"'d\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"n't\":\n",
        "                word_tokens[i] = 'not'\n",
        "            elif w == \"'m\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'re\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == 'gon':\n",
        "                word_tokens[i] = 'going to'\n",
        "            elif w == 'na':\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'s\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'ll\":\n",
        "                word_tokens[i] = ''\n",
        "            else:\n",
        "                pass\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = delete_contraction(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_at(word_tokens):  # remove '@' and words followed by @\n",
        "        j = 0\n",
        "        for i in range(len(word_tokens)):\n",
        "            try:\n",
        "                if word_tokens[j] == '@':\n",
        "                    del word_tokens[j + 1]\n",
        "                    del word_tokens[j]\n",
        "                    j += -1\n",
        "                j += 1\n",
        "            except:\n",
        "                break\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_at(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_numbers(word_tokens):\n",
        "      j = 0\n",
        "      for i in range(len(word_tokens)):\n",
        "        try:\n",
        "          if word_tokens[i-j].isnumeric():\n",
        "            del word_tokens[i-j]\n",
        "            j += 1\n",
        "        except:\n",
        "          break\n",
        "      return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_numbers(word_tokens)\n",
        "\n",
        "    cleaned_tweet = ' '.join(word_tokens).replace('_', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('#', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('/', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace(\"'\", ' ')\n",
        "\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n",
        "def clean_tweet_dataframe(df):  # Jaki's, not used but more efficient\n",
        "  data = df.string.values\n",
        "  clean_data = []\n",
        "  ######################## Denoising Texts #########################\n",
        "  letters = {'A', 'B', 'C', 'E', 'F', 'G', 'H', 'M', 'N', 'P', 'S', 'V', 'Y', 'a', 'b', 'v'}\n",
        "  symbols = {'!', '#', '$', '&', \"'\", '(', ')', '*', '+', '-', '_', '.', '/', ':', ';', '=', '?', '`', '|', '~',\n",
        "            '[', ']', '^',\n",
        "            '\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ', '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ',\n",
        "            '\\x89ÛÒ', 'Ûªs', 'Û_'}\n",
        "  start = time.time()\n",
        "  for d in data:\n",
        "      ws = d.split(' ')\n",
        "      for i, w in enumerate(ws):\n",
        "          # process 1: remove https and @\n",
        "          if ('http' in w) or ('@' in w):\n",
        "              ws[i] = ''\n",
        "          # process 2: remove single letters\n",
        "          if w in letters:\n",
        "              ws[i] = ''\n",
        "      new_d = ' '.join(ws)\n",
        "      # process 3: remove symbols and emotes\n",
        "      for s in symbols:\n",
        "          new_d = new_d.replace(s, ' ')\n",
        "      # process 4: replace multiple spaces with one space\n",
        "      new_d = ' '.join(new_d.split())\n",
        "      clean_data.append(new_d.lower())\n",
        "  end = time.time() - start\n",
        "  print(end)\n",
        "  print(len(clean_data))\n",
        "  return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlEDnkdWd1FI",
        "colab_type": "text"
      },
      "source": [
        "## Spell-Checker and Remove_Stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwj599LheNmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spellcheck(input_words):  # Additionally casts to LC and removes excess spaces as a nice side-effect of how it spell-checks.\n",
        "  speller = SpellChecker()\n",
        "  words = speller.split_words(input_words)\n",
        "  [speller.correction(w) for w in words]\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "def remove_stopwords(input_words):\n",
        "  stop_words = set(stopwords.words('english'))  # List of stopwords to remove\n",
        "  word_tokens = word_tokenize(input_words)\n",
        "  words = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "  return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZIoaXKfIwm",
        "colab_type": "text"
      },
      "source": [
        "## Other Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "souan2Eg7QEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dim(input_tweets):\n",
        "  m = 0\n",
        "  for i in range(len(input_tweets)):\n",
        "    word_tokens = word_tokenize(input_tweets[i])\n",
        "    m = max(m, len(word_tokens))\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_usfa7gtekIK",
        "colab_type": "text"
      },
      "source": [
        "## Data Import and Pre-Processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PZNUK32oqSD",
        "colab_type": "code",
        "outputId": "0f35b2c8-9444-4517-a99b-3fe560e252ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Try to import the cleaned sheet, if it doesn't exist, re-clean the raw data\n",
        "\n",
        "try:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  data = pd.read_csv(input_file).to_numpy()\n",
        "  print(\"Cleaned data successfully read from file!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Ugly_words_FULL - Sheet1.csv\"\n",
        "  df = pd.read_csv(input_file)\n",
        "  labels = df.columns.values\n",
        "  X = df.to_numpy()\n",
        "  Y = X[:1500, 0:6].astype(int) # Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News?\n",
        "  X = X[:1500, 6].astype(str)  # Dimensionless to pass strings/elements, expand to do matrix operations\n",
        "  # print(Y.shape)\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = clean_tweet(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = spellcheck(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = remove_stopwords(X[i])\n",
        "\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  X = np.expand_dims(X, axis=1)\n",
        "  data = np.concatenate((Y, X), axis=1)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.columns = labels\n",
        "\n",
        "  output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  df.to_csv(output_file, header=True, index=False)\n",
        "\n",
        "  print(\"Cleaned data successfully written to file!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaned data successfully read from file!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlHf_xz1GB6S",
        "colab_type": "code",
        "outputId": "b8ee40da-504b-4722-e219-e28493de514f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Oversample if desired (probably yes).\n",
        "y_start = 5\n",
        "y_end = 6\n",
        "\n",
        "oversample = True\n",
        "num_copies = 3  # Number of copies to add\n",
        "\n",
        "if oversample:\n",
        "  minority = data[:, :][data[:, y_start] == 1]\n",
        "  print(minority.shape)\n",
        "  \n",
        "  for i in range(num_copies):\n",
        "    np.random.shuffle(minority)\n",
        "    temp = minority\n",
        "    data = np.append(data, temp[:200, :], axis=0)\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(415, 7)\n",
            "(2100, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZt_7s5DRb0T",
        "colab_type": "code",
        "outputId": "8199126d-42f0-4fad-dd86-0c0b7fc2f33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Check sizes and split into train and test sets.\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News? [0:4]\n",
        "Y_train = data[:1100, y_start:y_end].astype(int).squeeze()\n",
        "X_train = data[:1100, 6].astype(str)\n",
        "Y_valid = data[1100:1600, y_start:y_end].astype(int).squeeze()\n",
        "X_valid = data[1100:1600, 6].astype(str)\n",
        "Y_test = data[1600:2100, y_start:y_end].astype(int).squeeze()\n",
        "X_test = data[1600:2100, 6].astype(str)\n",
        "\n",
        "# Print splits.\n",
        "print(len(X_train[Y_train==1]))\n",
        "print(len(X_valid[Y_valid==1]))\n",
        "print(len(X_test[Y_test==1]))\n",
        "\n",
        "# Tokenize and standardize lengths for training:\n",
        "token_len = find_dim(data[:, 6].astype(str))\n",
        "print(\"Padding Length: \" + str(token_len))\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.fit_on_texts(X_valid)\n",
        "# tokenizer.fit_on_texts(X_test)  # Don't fit over test since it's not part of the model.\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)  # Vectorizes into numbers\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)  # Vectorizes into numbers\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)  # Vectorizes into numbers\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=token_len, padding='pre', truncating='post')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "537\n",
            "240\n",
            "238\n",
            "Padding Length: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8Xp1mIeojP",
        "colab_type": "text"
      },
      "source": [
        "## Tensorization and Embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSk44mu0j5uD",
        "colab_type": "code",
        "outputId": "47dd6d86-e4e9-4b91-8c08-e8380e0f464e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Tensorization, shuffle and batch so the format is correct for embedding.\n",
        "X_train = tf.constant(X_train, dtype=tf.int64)\n",
        "Y_train = tf.constant(Y_train, dtype=tf.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(100).batch(100)\n",
        "print(train_data)\n",
        "\n",
        "X_valid = tf.constant(X_valid, dtype=tf.int64)\n",
        "Y_valid = tf.constant(Y_valid, dtype=tf.int64)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid)).shuffle(100).batch(100)\n",
        "print(valid_data)\n",
        "\n",
        "X_test = tf.constant(X_test, dtype=tf.int64)\n",
        "Y_test = tf.constant(Y_test, dtype=tf.int64)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(100).batch(100)\n",
        "print(test_data)\n",
        "\n",
        "# print(list(test_data.as_numpy_iterator()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEXAb3fe3qA",
        "colab_type": "text"
      },
      "source": [
        "## Build and Train Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc6Y16rCgXHz",
        "colab_type": "code",
        "outputId": "9ed473c2-34ca-4cef-b31b-ea43d2f7f0a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Define model layers with Dense over LSTM for Classification\n",
        "print(len(tokenizer.word_index))\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 1000))\n",
        "# Flatten to 1D to line up with subsequent layers since Embedding can take multiple and Dense cannot\n",
        "model.add(tf.keras.layers.LSTM(100, activation=\"relu\", return_sequences=False, recurrent_dropout=0.1))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4863\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 1000)        4864000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               440400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 5,334,801\n",
            "Trainable params: 5,334,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9nUvMtjijR",
        "colab_type": "code",
        "outputId": "1a788dd5-c189-45c4-ac77-b9c42407c6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define model parameters after defining layers with compile:\n",
        "tf.keras.backend.clear_session()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Run the fitting and train the model:\n",
        "# batch_size = 100 # Number of observations run per set\n",
        "history = model.fit(train_data, validation_data=valid_data, epochs=100, verbose=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 3s 228ms/step - loss: 0.6805 - accuracy: 0.6327 - val_loss: 0.6367 - val_accuracy: 0.7020\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 2s 178ms/step - loss: 0.5761 - accuracy: 0.8418 - val_loss: 0.5563 - val_accuracy: 0.6820\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 2s 182ms/step - loss: 0.2844 - accuracy: 0.9591 - val_loss: 0.3492 - val_accuracy: 0.8580\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 2s 182ms/step - loss: 0.0590 - accuracy: 0.9891 - val_loss: 0.4785 - val_accuracy: 0.9020\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 2s 191ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.4861 - val_accuracy: 0.8940\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 2s 184ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.5560 - val_accuracy: 0.8960\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.5485 - val_accuracy: 0.9100\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 2s 172ms/step - loss: 0.0068 - accuracy: 0.9964 - val_loss: 0.5794 - val_accuracy: 0.9120\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.5959 - val_accuracy: 0.9100\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.6413 - val_accuracy: 0.9080\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 2s 184ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.5641 - val_accuracy: 0.9100\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 2s 178ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.5977 - val_accuracy: 0.9100\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.5738 - val_accuracy: 0.9120\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 0.0049 - accuracy: 0.9973 - val_loss: 0.5720 - val_accuracy: 0.9100\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 2s 189ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.6196 - val_accuracy: 0.9120\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 2s 190ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.6160 - val_accuracy: 0.9120\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.5935 - val_accuracy: 0.9100\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 2s 172ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.6189 - val_accuracy: 0.9120\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.5950 - val_accuracy: 0.9100\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 2s 179ms/step - loss: 0.0055 - accuracy: 0.9964 - val_loss: 0.6273 - val_accuracy: 0.9120\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 2s 180ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.6418 - val_accuracy: 0.9120\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.6087 - val_accuracy: 0.9100\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 2s 169ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.6536 - val_accuracy: 0.9100\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.6193 - val_accuracy: 0.9100\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 2s 176ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.6415 - val_accuracy: 0.9100\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.6462 - val_accuracy: 0.9100\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 2s 186ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.6554 - val_accuracy: 0.9100\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.6184 - val_accuracy: 0.9100\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.6293 - val_accuracy: 0.9120\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 2s 175ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.6235 - val_accuracy: 0.9100\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 0.0055 - accuracy: 0.9973 - val_loss: 0.6477 - val_accuracy: 0.9100\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 2s 170ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.6364 - val_accuracy: 0.9120\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 2s 184ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.6119 - val_accuracy: 0.9100\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.6412 - val_accuracy: 0.9100\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 2s 180ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.6971 - val_accuracy: 0.9080\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 2s 196ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.6741 - val_accuracy: 0.9100\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.6526 - val_accuracy: 0.9100\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 2s 193ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.6988 - val_accuracy: 0.9120\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 2s 179ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.7245 - val_accuracy: 0.9100\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 2s 191ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.6832 - val_accuracy: 0.9100\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.6864 - val_accuracy: 0.9100\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 2s 173ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.7332 - val_accuracy: 0.9120\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.7280 - val_accuracy: 0.9100\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 2s 168ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.7345 - val_accuracy: 0.9120\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 2s 192ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.7138 - val_accuracy: 0.9100\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 2s 178ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.7134 - val_accuracy: 0.9100\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 2s 184ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.7405 - val_accuracy: 0.9100\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 2s 183ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.7182 - val_accuracy: 0.9100\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 2s 190ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.7300 - val_accuracy: 0.9100\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 2s 182ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.7663 - val_accuracy: 0.9100\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 2s 183ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.7554 - val_accuracy: 0.9100\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.7453 - val_accuracy: 0.9080\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 2s 189ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.7115 - val_accuracy: 0.9100\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 2s 183ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.7708 - val_accuracy: 0.9080\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 2s 184ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.7306 - val_accuracy: 0.9080\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 2s 190ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.7421 - val_accuracy: 0.9080\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 2s 172ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.7768 - val_accuracy: 0.9080\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 2s 175ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.8030 - val_accuracy: 0.9080\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 2s 176ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.8372 - val_accuracy: 0.9100\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 2s 185ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.9489 - val_accuracy: 0.9080\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 2s 194ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.9310 - val_accuracy: 0.9080\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 2s 189ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.9256 - val_accuracy: 0.9080\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 2s 182ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.8991 - val_accuracy: 0.9100\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 2s 184ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.9508 - val_accuracy: 0.9080\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 2s 191ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.0227 - val_accuracy: 0.9040\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.9832 - val_accuracy: 0.9080\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 2s 191ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 1.0083 - val_accuracy: 0.9080\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.1124 - val_accuracy: 0.9040\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.1389 - val_accuracy: 0.9040\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 2s 192ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.0821 - val_accuracy: 0.9080\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 2s 201ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 1.1462 - val_accuracy: 0.9040\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 2s 184ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 1.0553 - val_accuracy: 0.9080\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 2s 184ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.1356 - val_accuracy: 0.9060\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 2s 183ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.1517 - val_accuracy: 0.9060\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 2s 180ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.1131 - val_accuracy: 0.9080\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 2s 182ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.1725 - val_accuracy: 0.9040\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 2s 191ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.2183 - val_accuracy: 0.9060\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 2s 191ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.2149 - val_accuracy: 0.9060\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 1.2971 - val_accuracy: 0.9060\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 2s 187ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.1549 - val_accuracy: 0.9100\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 2s 183ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 1.3438 - val_accuracy: 0.9060\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 2s 177ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.4385 - val_accuracy: 0.9060\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 2s 183ms/step - loss: 0.0045 - accuracy: 0.9982 - val_loss: 1.2903 - val_accuracy: 0.9060\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 2s 173ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.2697 - val_accuracy: 0.9060\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 2s 190ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.4250 - val_accuracy: 0.9080\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 2s 195ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.4666 - val_accuracy: 0.9080\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 1.6511 - val_accuracy: 0.9060\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 2s 174ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 1.4555 - val_accuracy: 0.9060\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 2s 191ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.5324 - val_accuracy: 0.9080\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 2s 188ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.5612 - val_accuracy: 0.9080\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 2s 185ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.7110 - val_accuracy: 0.9060\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 2s 178ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 1.7419 - val_accuracy: 0.9080\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 2s 185ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.8653 - val_accuracy: 0.9060\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 2s 204ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 1.6244 - val_accuracy: 0.9060\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 2s 190ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 1.9674 - val_accuracy: 0.9080\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 2s 193ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 2.1115 - val_accuracy: 0.9060\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 2s 181ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 2.0226 - val_accuracy: 0.9080\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 2s 190ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 2.0772 - val_accuracy: 0.9060\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 2s 180ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 2.2428 - val_accuracy: 0.9040\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 2s 171ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 2.1860 - val_accuracy: 0.9060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS0CEZ1me5S4",
        "colab_type": "text"
      },
      "source": [
        "## Predict and Metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z95XMfjtiPP0",
        "colab_type": "code",
        "outputId": "7550511a-ea94-4267-f0ca-f7c5e4eac43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model.evaluate(test_data, verbose=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 - 0s - loss: 0.6305 - accuracy: 0.9200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZo1fqLiLj-",
        "colab_type": "code",
        "outputId": "07654cc3-be49-4161-ab18-9ce17fc89cc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.630\n",
            "accuracy: 0.920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSs6J5Aiyf3",
        "colab_type": "code",
        "outputId": "d44222aa-191c-4db4-d0ce-4f7d7160e6c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "test_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\n",
        "test_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\n",
        "test_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase1)))\n",
        "\n",
        "test_phrase2 = \"urgent news from israel\"\n",
        "test_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\n",
        "test_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase2)))\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\\ntest_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\\ntest_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase1)))\\n\\ntest_phrase2 = \"urgent news from israel\"\\ntest_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\\ntest_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase2)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLpc0QLRxQ4q",
        "colab_type": "code",
        "outputId": "4d22ce9f-6cbb-467f-965d-c1119dcb5e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "Y_hat = model.predict_classes(X_test).reshape(len(Y_test), 1)\n",
        "print(len(Y_hat))\n",
        "print(type(Y_hat))\n",
        "print(len(Y_hat[Y_hat==1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-be29c51753dd>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "500\n",
            "<class 'numpy.ndarray'>\n",
            "238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4uHbgPD7eGD",
        "colab_type": "code",
        "outputId": "16c1a458-0a65-4887-fa11-0868753a6bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Make Confusion Matrix\n",
        "plt.figure(figsize=(9, 6))\n",
        "y_true = pd.Series(Y_test.numpy().squeeze(), name=\"Actual Label\")\n",
        "y_pred = pd.Series(Y_hat.squeeze(), name=\"Predicted Label\")\n",
        "sns.heatmap(pd.crosstab(y_true, y_pred), annot=True, fmt=\"d\", linewidths=0.25)\n",
        "plt.ylim(len(set(Y_test.numpy())), 0)  # Fix limits, matplotlib bugged (ver. 3.11)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.0, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAFzCAYAAABM02E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkirMRECSpAozEZAkqcJMBCRJqrD/D2dgteb04lJAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ8ktCDu6tWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = np.expand_dims(X, axis=1)\n",
        "output = np.concatenate((Y_hat, Y_test.numpy().reshape(-1, 1), data[1600:2100, 6:7].reshape(-1, 1)), axis=1)\n",
        "df = pd.DataFrame(output)\n",
        "\n",
        "output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/NLP_Tweet_Classification_Results.csv\"\n",
        "df.to_csv(output_file, header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLNQXNktgyf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}