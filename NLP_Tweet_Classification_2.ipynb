{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Tweet Classification_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5a2fi7JePkG",
        "colab_type": "text"
      },
      "source": [
        "## Import Neccessary Packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyilNou3d0Ax",
        "colab_type": "code",
        "outputId": "e11efb19-20f7-430a-a664-819f16e02042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  # Returns split words, while tf vectorizes after splitting.\n",
        "\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9xAKlUqfqgn",
        "colab_type": "code",
        "outputId": "2cf7ee9a-5125-4bcd-bd77-0b5f2853f697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (46.0.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (2.1.0.dev202003290105)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.38.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.21.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly) (46.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8CjWN0FdTax",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQc0rSfb94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_tweet(uncleaned_tweet):\n",
        "    # Order of operations:\n",
        "    # 1. Remove symbols/emojis.\n",
        "    # 2. Tokenize for subsequent processes.\n",
        "    # 3. Remove http links.\n",
        "    # 4. Replace contractions with root words.\n",
        "    # 5. Remove @whatever.\n",
        "    # 6. Separate and Remove '_', '#', '/', ''' in that order.\n",
        "    # 7. Join and return.\n",
        "\n",
        "    # Identify junk to be removed.\n",
        "    symbols = ['*', '+', '...', '?', '`', '``', '[', ']', '(', ')', '-', '~', '|']\n",
        "    \n",
        "    emotes = ['\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ',\n",
        "              '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ', '\\x89ÛÒ', 'Ûªs']    \n",
        "    \n",
        "\n",
        "    def remove_symbols(uncleaned_tweet):\n",
        "        for s in symbols:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(s, ' ')\n",
        "        uncleaned_tweet = uncleaned_tweet.replace('&', ' and ')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_symbols(uncleaned_tweet)\n",
        "    \n",
        "    \n",
        "    def remove_emote(uncleaned_tweet):\n",
        "        for emote in emotes:\n",
        "            uncleaned_tweet = uncleaned_tweet.replace(emote, '')\n",
        "        return uncleaned_tweet\n",
        "\n",
        "\n",
        "    uncleaned_tweet = remove_emote(uncleaned_tweet)\n",
        "    word_tokens = word_tokenize(uncleaned_tweet)\n",
        "\n",
        "\n",
        "    def remove_links(word_tokens):\n",
        "        if 'http' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('http')]\n",
        "        if 'https' in word_tokens:\n",
        "            word_tokens = word_tokens[0:word_tokens.index('https')]\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_links(word_tokens)\n",
        "\n",
        "\n",
        "    def delete_contraction(word_tokens):\n",
        "        for i, w in enumerate(word_tokens):\n",
        "            if w == \"'d\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"n't\":\n",
        "                word_tokens[i] = 'not'\n",
        "            elif w == \"'m\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'re\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == 'gon':\n",
        "                word_tokens[i] = 'going to'\n",
        "            elif w == 'na':\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'s\":\n",
        "                word_tokens[i] = ''\n",
        "            elif w == \"'ll\":\n",
        "                word_tokens[i] = ''\n",
        "            else:\n",
        "                pass\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = delete_contraction(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_at(word_tokens):  # remove '@' and words followed by @\n",
        "        j = 0\n",
        "        for i in range(len(word_tokens)):\n",
        "            try:\n",
        "                if word_tokens[j] == '@':\n",
        "                    del word_tokens[j + 1]\n",
        "                    del word_tokens[j]\n",
        "                    j += -1\n",
        "                j += 1\n",
        "            except:\n",
        "                break\n",
        "        return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_at(word_tokens)\n",
        "\n",
        "\n",
        "    def remove_numbers(word_tokens):\n",
        "      j = 0\n",
        "      for i in range(len(word_tokens)):\n",
        "        try:\n",
        "          if word_tokens[i-j].isnumeric():\n",
        "            del word_tokens[i-j]\n",
        "            j += 1\n",
        "        except:\n",
        "          break\n",
        "      return word_tokens\n",
        "\n",
        "\n",
        "    word_tokens = remove_numbers(word_tokens)\n",
        "\n",
        "    cleaned_tweet = ' '.join(word_tokens).replace('_', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('#', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace('/', ' ')\n",
        "    cleaned_tweet = cleaned_tweet.replace(\"'\", ' ')\n",
        "\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "\n",
        "def clean_tweet_dataframe(df):  # Jaki's, not used but more efficient\n",
        "  data = df.string.values\n",
        "  clean_data = []\n",
        "  ######################## Denoising Texts #########################\n",
        "  letters = {'A', 'B', 'C', 'E', 'F', 'G', 'H', 'M', 'N', 'P', 'S', 'V', 'Y', 'a', 'b', 'v'}\n",
        "  symbols = {'!', '#', '$', '&', \"'\", '(', ')', '*', '+', '-', '_', '.', '/', ':', ';', '=', '?', '`', '|', '~',\n",
        "            '[', ']', '^',\n",
        "            '\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ', '\\x89Ûª', '\\x89Û÷', '\\x89ÛªS', 'RAZEDåÊ',\n",
        "            '\\x89ÛÒ', 'Ûªs', 'Û_'}\n",
        "  start = time.time()\n",
        "  for d in data:\n",
        "      ws = d.split(' ')\n",
        "      for i, w in enumerate(ws):\n",
        "          # process 1: remove https and @\n",
        "          if ('http' in w) or ('@' in w):\n",
        "              ws[i] = ''\n",
        "          # process 2: remove single letters\n",
        "          if w in letters:\n",
        "              ws[i] = ''\n",
        "      new_d = ' '.join(ws)\n",
        "      # process 3: remove symbols and emotes\n",
        "      for s in symbols:\n",
        "          new_d = new_d.replace(s, ' ')\n",
        "      # process 4: replace multiple spaces with one space\n",
        "      new_d = ' '.join(new_d.split())\n",
        "      clean_data.append(new_d.lower())\n",
        "  end = time.time() - start\n",
        "  print(end)\n",
        "  print(len(clean_data))\n",
        "  return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlEDnkdWd1FI",
        "colab_type": "text"
      },
      "source": [
        "## Spell-Checker and Remove_Stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwj599LheNmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spellcheck(input_words):  # Additionally casts to LC and removes excess spaces as a nice side-effect of how it spell-checks.\n",
        "  speller = SpellChecker()\n",
        "  words = speller.split_words(input_words)\n",
        "  [speller.correction(w) for w in words]\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "def remove_stopwords(input_words):\n",
        "  stop_words = set(stopwords.words('english'))  # List of stopwords to remove\n",
        "  word_tokens = word_tokenize(input_words)\n",
        "  words = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "  return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZIoaXKfIwm",
        "colab_type": "text"
      },
      "source": [
        "## Other Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "souan2Eg7QEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_dim(input_tweets):\n",
        "  m = 0\n",
        "  for i in range(len(input_tweets)):\n",
        "    word_tokens = word_tokenize(input_tweets[i])\n",
        "    m = max(m, len(word_tokens))\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_usfa7gtekIK",
        "colab_type": "text"
      },
      "source": [
        "## Data Import and Pre-Processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PZNUK32oqSD",
        "colab_type": "code",
        "outputId": "c8e9779c-1b31-4308-d547-93e1d7ee07af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Try to import the cleaned sheet, if it doesn't exist, re-clean the raw data\n",
        "\n",
        "try:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  data = pd.read_csv(input_file).to_numpy()\n",
        "  print(\"Cleaned data successfully read from file!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  input_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Ugly_words_FULL - Sheet1.csv\"\n",
        "  df = pd.read_csv(input_file)\n",
        "  labels = df.columns.values\n",
        "  X = df.to_numpy()\n",
        "  Y = X[:1500, 0:6].astype(int) # Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News?\n",
        "  X = X[:1500, 6].astype(str)  # Dimensionless to pass strings/elements, expand to do matrix operations\n",
        "  # print(Y.shape)\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = clean_tweet(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = spellcheck(X[i])\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    X[i] = remove_stopwords(X[i])\n",
        "\n",
        "  # print(X.shape)\n",
        "  # print(X)\n",
        "\n",
        "  X = np.expand_dims(X, axis=1)\n",
        "  data = np.concatenate((Y, X), axis=1)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.columns = labels\n",
        "\n",
        "  output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/Cleaned_words.csv\"\n",
        "  df.to_csv(output_file, header=True, index=False)\n",
        "\n",
        "  print(\"Cleaned data successfully written to file!\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaned data successfully read from file!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlHf_xz1GB6S",
        "colab_type": "code",
        "outputId": "cf99ce9a-afd1-4232-9b94-f52e6e972fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Oversample if desired (probably yes).\n",
        "y_start = 2\n",
        "y_end = 3\n",
        "\n",
        "oversample = True\n",
        "num_copies = 3  # Number of copies to add\n",
        "\n",
        "if oversample:\n",
        "  minority = data[:, :][data[:, y_start] == 1]\n",
        "  print(minority.shape)\n",
        "  \n",
        "  for i in range(num_copies):\n",
        "    np.random.shuffle(minority)\n",
        "    temp = minority\n",
        "    data = np.append(data, temp[:200, :], axis=0)\n",
        "\n",
        "print(data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(270, 7)\n",
            "(2100, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZt_7s5DRb0T",
        "colab_type": "code",
        "outputId": "1e37d89d-3209-43cf-91c0-618655032f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Check sizes and split into train and test sets.\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Labels: Neg/Pos, Is Ad/Spam?, Is Review?, Is News? [0:4]\n",
        "Y_train = data[:1100, y_start:y_end].astype(int).squeeze()\n",
        "X_train = data[:1100, 6].astype(str)\n",
        "Y_valid = data[1100:1600, y_start:y_end].astype(int).squeeze()\n",
        "X_valid = data[1100:1600, 6].astype(str)\n",
        "Y_test = data[1600:2100, y_start:y_end].astype(int).squeeze()\n",
        "X_test = data[1600:2100, 6].astype(str)\n",
        "\n",
        "# Print splits.\n",
        "print(len(X_train[Y_train==1]))\n",
        "print(len(X_valid[Y_valid==1]))\n",
        "print(len(X_test[Y_test==1]))\n",
        "\n",
        "# Tokenize and standardize lengths for training:\n",
        "token_len = find_dim(data[:, 6].astype(str))\n",
        "print(\"Padding Length: \" + str(token_len))\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.fit_on_texts(X_valid)\n",
        "# tokenizer.fit_on_texts(X_test)  # Don't fit over test since it's not part of the model.\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)  # Vectorizes into numbers\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)  # Vectorizes into numbers\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)  # Vectorizes into numbers\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=token_len, padding='pre', truncating='post')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "471\n",
            "210\n",
            "189\n",
            "Padding Length: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8Xp1mIeojP",
        "colab_type": "text"
      },
      "source": [
        "## Tensorization and Embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSk44mu0j5uD",
        "colab_type": "code",
        "outputId": "04753a07-a317-40f8-8126-9d6e36350925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Tensorization, shuffle and batch so the format is correct for embedding.\n",
        "X_train = tf.constant(X_train, dtype=tf.int64)\n",
        "Y_train = tf.constant(Y_train, dtype=tf.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(100).batch(100)\n",
        "print(train_data)\n",
        "\n",
        "X_valid = tf.constant(X_valid, dtype=tf.int64)\n",
        "Y_valid = tf.constant(Y_valid, dtype=tf.int64)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid)).shuffle(100).batch(100)\n",
        "print(valid_data)\n",
        "\n",
        "X_test = tf.constant(X_test, dtype=tf.int64)\n",
        "Y_test = tf.constant(Y_test, dtype=tf.int64)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(100).batch(100)\n",
        "print(test_data)\n",
        "\n",
        "# print(list(test_data.as_numpy_iterator()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 25), (None,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTEXAb3fe3qA",
        "colab_type": "text"
      },
      "source": [
        "## Build and Train Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc6Y16rCgXHz",
        "colab_type": "code",
        "outputId": "3ab052b2-ef6e-4a7d-eceb-80ddffe3685a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Define model layers with Dense over LSTM for Classification\n",
        "print(len(tokenizer.word_index))\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 1000))\n",
        "# Flatten to 1D to line up with subsequent layers since Embedding can take multiple and Dense cannot\n",
        "model.add(tf.keras.layers.LSTM(100, activation=\"relu\", return_sequences=False, recurrent_dropout=0.1))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation =\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4814\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 1000)        4815000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               440400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 5,285,801\n",
            "Trainable params: 5,285,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh9nUvMtjijR",
        "colab_type": "code",
        "outputId": "e5f007bd-87dc-42cb-f32a-a4b995c87b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define model parameters after defining layers with compile:\n",
        "tf.keras.backend.clear_session()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Run the fitting and train the model:\n",
        "# batch_size = 100 # Number of observations run per set\n",
        "history = model.fit(train_data, validation_data=valid_data, epochs=100, verbose=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 0.6858 - accuracy: 0.5582 - val_loss: 0.6686 - val_accuracy: 0.5800\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 2s 136ms/step - loss: 0.5718 - accuracy: 0.6482 - val_loss: 0.5018 - val_accuracy: 0.8360\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.2332 - accuracy: 0.9436 - val_loss: 0.3889 - val_accuracy: 0.8580\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0790 - accuracy: 0.9909 - val_loss: 0.4461 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0273 - accuracy: 0.9909 - val_loss: 0.7381 - val_accuracy: 0.8260\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0183 - accuracy: 0.9927 - val_loss: 0.7253 - val_accuracy: 0.8460\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0125 - accuracy: 0.9927 - val_loss: 0.7349 - val_accuracy: 0.8520\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0131 - accuracy: 0.9927 - val_loss: 0.7064 - val_accuracy: 0.8520\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0113 - accuracy: 0.9927 - val_loss: 0.7183 - val_accuracy: 0.8480\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0106 - accuracy: 0.9918 - val_loss: 0.7356 - val_accuracy: 0.8500\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0100 - accuracy: 0.9927 - val_loss: 0.7457 - val_accuracy: 0.8460\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0100 - accuracy: 0.9936 - val_loss: 0.7693 - val_accuracy: 0.8600\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0099 - accuracy: 0.9927 - val_loss: 0.7844 - val_accuracy: 0.8580\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0100 - accuracy: 0.9927 - val_loss: 0.7997 - val_accuracy: 0.8560\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0098 - accuracy: 0.9927 - val_loss: 0.7928 - val_accuracy: 0.8440\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0102 - accuracy: 0.9918 - val_loss: 0.8212 - val_accuracy: 0.8520\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0096 - accuracy: 0.9918 - val_loss: 0.8178 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0099 - accuracy: 0.9909 - val_loss: 0.8297 - val_accuracy: 0.8580\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0101 - accuracy: 0.9909 - val_loss: 0.8394 - val_accuracy: 0.8520\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0098 - accuracy: 0.9909 - val_loss: 0.8441 - val_accuracy: 0.8540\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0098 - accuracy: 0.9927 - val_loss: 0.8537 - val_accuracy: 0.8520\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0095 - accuracy: 0.9936 - val_loss: 0.8505 - val_accuracy: 0.8480\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0097 - accuracy: 0.9909 - val_loss: 0.8682 - val_accuracy: 0.8500\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0095 - accuracy: 0.9909 - val_loss: 0.8700 - val_accuracy: 0.8540\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0096 - accuracy: 0.9918 - val_loss: 0.8809 - val_accuracy: 0.8520\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0097 - accuracy: 0.9909 - val_loss: 0.8826 - val_accuracy: 0.8540\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0097 - accuracy: 0.9927 - val_loss: 0.8769 - val_accuracy: 0.8460\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0098 - accuracy: 0.9918 - val_loss: 0.8908 - val_accuracy: 0.8540\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0095 - accuracy: 0.9909 - val_loss: 0.8894 - val_accuracy: 0.8560\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0094 - accuracy: 0.9918 - val_loss: 0.8859 - val_accuracy: 0.8440\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0097 - accuracy: 0.9909 - val_loss: 0.8919 - val_accuracy: 0.8580\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0095 - accuracy: 0.9909 - val_loss: 0.9053 - val_accuracy: 0.8560\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0094 - accuracy: 0.9927 - val_loss: 0.9057 - val_accuracy: 0.8560\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0095 - accuracy: 0.9909 - val_loss: 0.9106 - val_accuracy: 0.8560\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0095 - accuracy: 0.9909 - val_loss: 0.9126 - val_accuracy: 0.8540\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0096 - accuracy: 0.9909 - val_loss: 0.9085 - val_accuracy: 0.8560\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0097 - accuracy: 0.9909 - val_loss: 0.9219 - val_accuracy: 0.8580\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0094 - accuracy: 0.9909 - val_loss: 0.9126 - val_accuracy: 0.8520\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0096 - accuracy: 0.9918 - val_loss: 0.9222 - val_accuracy: 0.8540\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0095 - accuracy: 0.9918 - val_loss: 0.9215 - val_accuracy: 0.8540\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 2s 136ms/step - loss: 0.0098 - accuracy: 0.9909 - val_loss: 0.9245 - val_accuracy: 0.8560\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0094 - accuracy: 0.9927 - val_loss: 0.9207 - val_accuracy: 0.8560\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0095 - accuracy: 0.9927 - val_loss: 0.9245 - val_accuracy: 0.8580\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0093 - accuracy: 0.9909 - val_loss: 0.9302 - val_accuracy: 0.8540\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0095 - accuracy: 0.9918 - val_loss: 0.9364 - val_accuracy: 0.8560\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0094 - accuracy: 0.9927 - val_loss: 0.9282 - val_accuracy: 0.8440\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0094 - accuracy: 0.9918 - val_loss: 0.9493 - val_accuracy: 0.8580\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0094 - accuracy: 0.9936 - val_loss: 0.9475 - val_accuracy: 0.8580\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0094 - accuracy: 0.9918 - val_loss: 0.9421 - val_accuracy: 0.8420\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0095 - accuracy: 0.9918 - val_loss: 0.9477 - val_accuracy: 0.8440\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0094 - accuracy: 0.9936 - val_loss: 0.9628 - val_accuracy: 0.8580\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0094 - accuracy: 0.9936 - val_loss: 0.9493 - val_accuracy: 0.8460\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0094 - accuracy: 0.9918 - val_loss: 0.9464 - val_accuracy: 0.8460\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0093 - accuracy: 0.9927 - val_loss: 0.9640 - val_accuracy: 0.8580\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0093 - accuracy: 0.9927 - val_loss: 0.9670 - val_accuracy: 0.8580\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0092 - accuracy: 0.9936 - val_loss: 0.9580 - val_accuracy: 0.8520\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0094 - accuracy: 0.9945 - val_loss: 0.9635 - val_accuracy: 0.8480\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 2s 136ms/step - loss: 0.0093 - accuracy: 0.9918 - val_loss: 0.9826 - val_accuracy: 0.8500\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0093 - accuracy: 0.9918 - val_loss: 0.9952 - val_accuracy: 0.8580\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0093 - accuracy: 0.9936 - val_loss: 0.9921 - val_accuracy: 0.8500\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0092 - accuracy: 0.9918 - val_loss: 0.9933 - val_accuracy: 0.8520\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0094 - accuracy: 0.9936 - val_loss: 1.0026 - val_accuracy: 0.8520\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0093 - accuracy: 0.9909 - val_loss: 1.0111 - val_accuracy: 0.8500\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0094 - accuracy: 0.9909 - val_loss: 1.0232 - val_accuracy: 0.8560\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0093 - accuracy: 0.9918 - val_loss: 1.0345 - val_accuracy: 0.8540\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0092 - accuracy: 0.9918 - val_loss: 1.0351 - val_accuracy: 0.8540\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0093 - accuracy: 0.9918 - val_loss: 1.0461 - val_accuracy: 0.8540\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0093 - accuracy: 0.9909 - val_loss: 1.0648 - val_accuracy: 0.8560\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0092 - accuracy: 0.9927 - val_loss: 1.0718 - val_accuracy: 0.8580\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0092 - accuracy: 0.9927 - val_loss: 1.0798 - val_accuracy: 0.8600\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0091 - accuracy: 0.9936 - val_loss: 1.1015 - val_accuracy: 0.8600\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0093 - accuracy: 0.9918 - val_loss: 1.1193 - val_accuracy: 0.8600\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0093 - accuracy: 0.9918 - val_loss: 1.1256 - val_accuracy: 0.8540\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0092 - accuracy: 0.9918 - val_loss: 1.1520 - val_accuracy: 0.8600\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0093 - accuracy: 0.9945 - val_loss: 1.1832 - val_accuracy: 0.8580\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0091 - accuracy: 0.9918 - val_loss: 1.2085 - val_accuracy: 0.8600\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0093 - accuracy: 0.9909 - val_loss: 1.2379 - val_accuracy: 0.8620\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0092 - accuracy: 0.9927 - val_loss: 1.2368 - val_accuracy: 0.8620\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0091 - accuracy: 0.9927 - val_loss: 1.2360 - val_accuracy: 0.8560\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0092 - accuracy: 0.9909 - val_loss: 1.2563 - val_accuracy: 0.8620\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0091 - accuracy: 0.9927 - val_loss: 1.2772 - val_accuracy: 0.8620\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0091 - accuracy: 0.9936 - val_loss: 1.3060 - val_accuracy: 0.8600\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0091 - accuracy: 0.9918 - val_loss: 1.3317 - val_accuracy: 0.8640\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0091 - accuracy: 0.9927 - val_loss: 1.3616 - val_accuracy: 0.8640\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0091 - accuracy: 0.9945 - val_loss: 1.4008 - val_accuracy: 0.8620\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0090 - accuracy: 0.9927 - val_loss: 1.4344 - val_accuracy: 0.8560\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0092 - accuracy: 0.9945 - val_loss: 1.4644 - val_accuracy: 0.8560\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0090 - accuracy: 0.9918 - val_loss: 1.4310 - val_accuracy: 0.8640\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0091 - accuracy: 0.9927 - val_loss: 1.4316 - val_accuracy: 0.8640\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0092 - accuracy: 0.9945 - val_loss: 1.4583 - val_accuracy: 0.8640\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0092 - accuracy: 0.9909 - val_loss: 1.4875 - val_accuracy: 0.8640\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0091 - accuracy: 0.9918 - val_loss: 1.5098 - val_accuracy: 0.8660\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0091 - accuracy: 0.9918 - val_loss: 1.5338 - val_accuracy: 0.8640\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0091 - accuracy: 0.9909 - val_loss: 1.5652 - val_accuracy: 0.8680\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0091 - accuracy: 0.9936 - val_loss: 1.5968 - val_accuracy: 0.8680\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0090 - accuracy: 0.9945 - val_loss: 1.6421 - val_accuracy: 0.8680\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 6.9903 - accuracy: 0.9900 - val_loss: 0.7286 - val_accuracy: 0.8620\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0941 - accuracy: 0.9655 - val_loss: 0.6667 - val_accuracy: 0.7800\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0392 - accuracy: 0.9845 - val_loss: 0.6483 - val_accuracy: 0.8660\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0147 - accuracy: 0.9927 - val_loss: 0.9859 - val_accuracy: 0.8420\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS0CEZ1me5S4",
        "colab_type": "text"
      },
      "source": [
        "## Predict and Metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z95XMfjtiPP0",
        "colab_type": "code",
        "outputId": "abeb7649-791f-4e9c-b813-d475dbd15384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results = model.evaluate(test_data, verbose=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 - 0s - loss: 1.2149 - accuracy: 0.8100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZo1fqLiLj-",
        "colab_type": "code",
        "outputId": "555ba8e4-ad37-4060-c8d1-89f290dcb60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 1.215\n",
            "accuracy: 0.810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSs6J5Aiyf3",
        "colab_type": "code",
        "outputId": "c5671fd8-6b16-4bbe-dc7f-2d2286157ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "test_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\n",
        "test_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\n",
        "test_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase1)))\n",
        "\n",
        "test_phrase2 = \"urgent news from israel\"\n",
        "test_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\n",
        "test_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding='pre', truncating='post')\n",
        "print(np.argmax(model.predict(test_phrase2)))\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_phrase1 = \"buy new and improved shoes today! this is spam! and purses!\"\\ntest_phrase1 = tokenizer.texts_to_sequences(test_phrase1)  # Vectorizes into numbers\\ntest_phrase1 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase1, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase1)))\\n\\ntest_phrase2 = \"urgent news from israel\"\\ntest_phrase2 = tokenizer.texts_to_sequences(test_phrase2)  # Vectorizes into numbers\\ntest_phrase2 = tf.keras.preprocessing.sequence.pad_sequences(test_phrase2, maxlen=token_len, padding=\\'pre\\', truncating=\\'post\\')\\nprint(np.argmax(model.predict(test_phrase2)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLpc0QLRxQ4q",
        "colab_type": "code",
        "outputId": "2e14291a-63ee-4d54-c133-143ac717022e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "Y_hat = model.predict_classes(X_test).reshape(len(Y_test), 1)\n",
        "print(len(Y_hat))\n",
        "print(type(Y_hat))\n",
        "print(len(Y_hat[Y_hat==1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-be29c51753dd>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "500\n",
            "<class 'numpy.ndarray'>\n",
            "238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4uHbgPD7eGD",
        "colab_type": "code",
        "outputId": "3b453807-1361-4ad4-a1f0-6a7d9a7ab81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# Make Confusion Matrix\n",
        "plt.figure(figsize=(9, 6))\n",
        "y_true = pd.Series(Y_test.numpy().squeeze(), name=\"Actual Label\")\n",
        "y_pred = pd.Series(Y_hat.squeeze(), name=\"Predicted Label\")\n",
        "sns.heatmap(pd.crosstab(y_true, y_pred), annot=True, fmt=\"d\", linewidths=0.25)\n",
        "plt.ylim(len(set(Y_test.numpy())), 0)  # Fix limits, matplotlib bugged (ver. 3.11)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.0, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAFzCAYAAABM02E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUoWZCEiSVGH/HzxEYDGWyzMLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 648x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ8ktCDu6tWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = np.expand_dims(X, axis=1)\n",
        "output = np.concatenate((Y_hat, Y_test.numpy().reshape(-1, 1), data[1600:2100, 6:7].reshape(-1, 1)), axis=1)\n",
        "df = pd.DataFrame(output)\n",
        "\n",
        "output_file = \"/content/drive/My Drive/Colab Notebooks/NLP Tweet Classification/NLP_Tweet_Classification_Results.csv\"\n",
        "df.to_csv(output_file, header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLNQXNktgyf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}